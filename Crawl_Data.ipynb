{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ‚öôÔ∏è Install Libraries and Download Dataset\n",
        "The master's thesis used a much larger dataset of Reddit comments. Here, this is used to reduce the download time. The rest of the procedure is analogous to the master thesis."
      ],
      "metadata": {
        "id": "Us89bVllBS3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm pandas zstandard transformers evaluate\n",
        "\n",
        "!wget -O RC_2019-04.zst https://zenodo.org/record/3608135/files/RC_2019-04.zst?download=1"
      ],
      "metadata": {
        "id": "5vSBwrjHBZvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unpack the file"
      ],
      "metadata": {
        "id": "J69Z4rCXDFi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zstandard as zstd\n",
        "\n",
        "your_filename = \"...\"\n",
        "with open('RC_2019-04.zst', \"rb\") as f:\n",
        "    data = f.read()\n",
        "\n",
        "dctx = zstd.ZstdDecompressor()\n",
        "decompressed = dctx.decompress(data)\n"
      ],
      "metadata": {
        "id": "68dcD_A-DEKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üë©‚Äçüíª Extract comments\n",
        "\n",
        "You can specify the subreddits whose comments should be collected at the bottom of the code"
      ],
      "metadata": {
        "id": "ZMn_FU2qCKsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# set the list of subreddits to filter by\n",
        "subreddits = [\"antiwork\", \"atheism\", \"Conservative\", \"conspiracy\", \"dankmemes\", \"gaybros\", \"leagueoflegends\",\n",
        "              \"lgbt\", \"Libertarian\", \"linguistics\", \"MensRights\", \"news\", \"offbeat\", \"PoliticalCompassMemes\",\n",
        "              \"politics\",\n",
        "              \"teenagers\", \"TrueReddit\", \"TwoXChromosomes\", \"wallstreetbets\", \"worldnews\"]\n",
        "\n",
        "# set the number of comments to randomly select\n",
        "num_comments = 5000\n",
        "\n",
        "\n",
        "# define function to extract relevant fields from comment\n",
        "def extract_fields(comment):\n",
        "    fields = {}\n",
        "    fields['subreddit'] = comment['subreddit']\n",
        "    fields['id'] = comment['id']\n",
        "    fields['submission_id'] = comment['link_id'].split('_')[1]\n",
        "    fields['body'] = comment['body']\n",
        "    fields['created_utc'] = comment['created_utc']\n",
        "    fields['parent_id'] = comment['parent_id']\n",
        "    fields['permalink'] = 'https://www.reddit.com' + comment['permalink']\n",
        "    return fields\n",
        "\n",
        "\n",
        "subreddit_groups = {}\n",
        "for subreddit in subreddits:\n",
        "    subreddit_groups[subreddit] = []\n",
        "\n",
        "print(\"About to parse the whole file (this may take a while)\")\n",
        "# open the file and read in the comments\n",
        "with open('RC_2023-03', 'r') as f:\n",
        "    for line in tqdm(f):\n",
        "        comment = json.loads(line)\n",
        "        if comment['subreddit'] in subreddits:\n",
        "            subreddit_groups[comment['subreddit']].append(extract_fields(comment))\n",
        "with open(\"gathered_comments.json\", \"w\") as r:\n",
        "    json.dump(subreddit_groups, r)\n",
        "print(\"Parsed the whole file\")\n"
      ],
      "metadata": {
        "id": "FPvxZgZgA9tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìã Create a .csv file with a specified number of random comments from the selected subreddits"
      ],
      "metadata": {
        "id": "OW72UessDcay"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8C49UYILApRU"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "subreddits = [\"antiwork\", \"atheism\", \"Conservative\", \"conspiracy\", \"dankmemes\", \"gaybros\", \"leagueoflegends\",\n",
        "              \"lgbt\", \"Libertarian\", \"linguistics\", \"MensRights\", \"news\", \"offbeat\", \"PoliticalCompassMemes\",\n",
        "              \"politics\",\n",
        "              \"teenagers\", \"TrueReddit\", \"TwoXChromosomes\", \"wallstreetbets\", \"worldnews\"]\n",
        "\n",
        "gathered_comments_file = open(\"gathered_comments.json\")\n",
        "gathered_comments = json.load(gathered_comments_file)\n",
        "\n",
        "num_comments = 3000\n",
        "\n",
        "selected_comments = []\n",
        "for subreddit in tqdm(subreddits):\n",
        "    number_of_comments = len(gathered_comments[subreddit])\n",
        "    if number_of_comments < num_comments:\n",
        "        selected_comments.extend(gathered_comments[subreddit])\n",
        "        continue\n",
        "    selected_comments.extend(random.sample(gathered_comments[subreddit], num_comments))\n",
        "\n",
        "print(\"Convert the list to a data frame\")\n",
        "# create a DataFrame from the selected comments\n",
        "df = pd.DataFrame(selected_comments)\n",
        "print(\"List converted\")\n",
        "\n",
        "# save the DataFrame to a CSV file\n",
        "df.to_csv('selected_comments.csv', index=False)\n",
        "print(\"file saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üßë‚Äçüî¨ Filter data"
      ],
      "metadata": {
        "id": "4Q71XbZ04N3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "result_file_name = 'filtered_comments.csv'\n",
        "\n",
        "min_length = 10  # token\n",
        "max_length = 512  # token\n",
        "\n",
        "data = pandas.io.parsers.read_csv(\"selected_comments.cs\")\n",
        "\n",
        "url_pattern = re.compile(r'https?:\\/\\/\\S+')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "\n",
        "filtered_data = []\n",
        "for index, row in tqdm(data.iterrows(), total=data.shape[0], desc='Filter Comments'):\n",
        "    comment = row.body\n",
        "    # remove all whitespace characters and line breaks and other unnecessary characters\n",
        "    comment = comment.replace('\\n', ' ').replace('\\\\n', ' ')\n",
        "    if comment.startswith('>'):\n",
        "        comment = comment.replace('>', '')\n",
        "    comment = ' '.join(comment.split())\n",
        "\n",
        "    # check length\n",
        "    tokenized_comment = tokenizer.tokenize(comment)\n",
        "    if len(tokenized_comment) > max_length or len(tokenized_comment) < min_length:\n",
        "        continue\n",
        "    # check content\n",
        "    if comment == \"[removed]\" or comment == \"[deleted]\":\n",
        "        continue\n",
        "    # remove comments containing urls\n",
        "    if url_pattern.search(comment):\n",
        "        continue\n",
        "    filtered_data.append({'subreddit': row.subreddit, 'id': row.id, 'submission_id': row.submission_id,\n",
        "                          'body': comment, 'created_utc': row.created_utc, 'parent_id': row.parent_id,\n",
        "                          'permalink': row.permalink})\n",
        "\n",
        "filtered_data_df = pandas.DataFrame(filtered_data)\n",
        "filtered_data_df.to_csv(result_file_name, sep=\",\", encoding=\"utf-8\")\n"
      ],
      "metadata": {
        "id": "DTtXsy704NN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üßë‚Äçüè≠ Add perplexities to the filtered data"
      ],
      "metadata": {
        "id": "cxKXnikY5e1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "result_file_name = 'filtered_comments_with_ppl.csv'\n",
        "\n",
        "data = pandas.io.parsers.read_csv(\"filtered_comments.csv\")\n",
        "\n",
        "perplexity_model = evaluate.load(\"perplexity\", module_type=\"measurement\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "\n",
        "comment_objects = []\n",
        "prepared_comments = []\n",
        "for index, row in tqdm(data.iterrows(), total=data.shape[0], desc='Filter Comments'):\n",
        "    comment = row.body\n",
        "    token_size = len(tokenizer.tokenize(comment))\n",
        "    if token_size > 1024 and tokenizer < 10:\n",
        "        continue\n",
        "    comment_objects.append({'subreddit': row.subreddit, 'id': row.id, 'submission_id': row.submission_id,\n",
        "                            'body': comment, 'created_utc': row.created_utc, 'parent_id': row.parent_id,\n",
        "                            'permalink': row.permalink, 'token_size': token_size})\n",
        "    prepared_comments.append(comment)\n",
        "\n",
        "perplexity_results = perplexity_model.compute(data=prepared_comments, model_id='gpt2')\n",
        "print(perplexity_results)\n",
        "\n",
        "\n",
        "filtered_data = []\n",
        "for i in range(len(perplexity_results['perplexities'])):\n",
        "    comment_objects[i]['perplexity'] = perplexity_results['perplexities'][i]\n",
        "    filtered_data.append(comment_objects[i])\n",
        "\n",
        "filtered_data_df = pandas.DataFrame(filtered_data)\n",
        "filtered_data_df.to_csv(result_file_name, sep=\",\", encoding=\"utf-8\")\n"
      ],
      "metadata": {
        "id": "HnzlzLax5u_G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
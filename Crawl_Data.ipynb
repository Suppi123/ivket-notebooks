{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ‚öôÔ∏è Install Libraries and Download Dataset\n",
        "The master's thesis used a much larger dataset of Reddit comments. Here, this is used to reduce the download time. The rest of the procedure is analogous to the master thesis."
      ],
      "metadata": {
        "id": "Us89bVllBS3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm pandas zstandard\n",
        "\n",
        "!wget -O RC_2019-04.zst https://zenodo.org/record/3608135/files/RC_2019-04.zst?download=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vSBwrjHBZvj",
        "outputId": "ae9f7f62-a5af-4ba7-a887-9846c4954409"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "--2023-06-17 20:54:25--  https://zenodo.org/record/3608135/files/RC_2019-04.zst?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.124.72\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.124.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15531201485 (14G) [application/octet-stream]\n",
            "Saving to: ‚ÄòRC_2019-04.zst‚Äô\n",
            "\n",
            "RC_2019-04.zst      100%[===================>]  14.46G  12.2MB/s    in 21m 21s \n",
            "\n",
            "2023-06-17 21:15:48 (11.6 MB/s) - ‚ÄòRC_2019-04.zst‚Äô saved [15531201485/15531201485]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unpack the file"
      ],
      "metadata": {
        "id": "J69Z4rCXDFi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zstandard as zstd\n",
        "\n",
        "your_filename = \"...\"\n",
        "with open('RC_2019-04.zst', \"rb\") as f:\n",
        "    data = f.read()\n",
        "\n",
        "dctx = zstd.ZstdDecompressor()\n",
        "decompressed = dctx.decompress(data)\n"
      ],
      "metadata": {
        "id": "68dcD_A-DEKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üë©‚Äçüíª Extract comments\n",
        "\n",
        "You can specify the subreddits whose comments should be collected at the bottom of the code"
      ],
      "metadata": {
        "id": "ZMn_FU2qCKsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# set the list of subreddits to filter by\n",
        "subreddits = [\"antiwork\", \"atheism\", \"Conservative\", \"conspiracy\", \"dankmemes\", \"gaybros\", \"leagueoflegends\",\n",
        "              \"lgbt\", \"Libertarian\", \"linguistics\", \"MensRights\", \"news\", \"offbeat\", \"PoliticalCompassMemes\",\n",
        "              \"politics\",\n",
        "              \"teenagers\", \"TrueReddit\", \"TwoXChromosomes\", \"wallstreetbets\", \"worldnews\"]\n",
        "\n",
        "# set the number of comments to randomly select\n",
        "num_comments = 5000\n",
        "\n",
        "\n",
        "# define function to extract relevant fields from comment\n",
        "def extract_fields(comment):\n",
        "    fields = {}\n",
        "    fields['subreddit'] = comment['subreddit']\n",
        "    fields['id'] = comment['id']\n",
        "    fields['submission_id'] = comment['link_id'].split('_')[1]\n",
        "    fields['body'] = comment['body']\n",
        "    fields['created_utc'] = comment['created_utc']\n",
        "    fields['parent_id'] = comment['parent_id']\n",
        "    fields['permalink'] = 'https://www.reddit.com' + comment['permalink']\n",
        "    return fields\n",
        "\n",
        "\n",
        "subreddit_groups = {}\n",
        "for subreddit in subreddits:\n",
        "    subreddit_groups[subreddit] = []\n",
        "\n",
        "print(\"About to parse the whole file (this may take a while)\")\n",
        "# open the file and read in the comments\n",
        "with open('RC_2023-03', 'r') as f:\n",
        "    for line in tqdm(f):\n",
        "        comment = json.loads(line)\n",
        "        if comment['subreddit'] in subreddits:\n",
        "            subreddit_groups[comment['subreddit']].append(extract_fields(comment))\n",
        "with open(\"gathered_comments.json\", \"w\") as r:\n",
        "    json.dump(subreddit_groups, r)\n",
        "print(\"Parsed the whole file\")\n"
      ],
      "metadata": {
        "id": "FPvxZgZgA9tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìã Create a .csv file with a specified number of random comments from the selected subreddits"
      ],
      "metadata": {
        "id": "OW72UessDcay"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8C49UYILApRU"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "subreddits = [\"antiwork\", \"atheism\", \"Conservative\", \"conspiracy\", \"dankmemes\", \"gaybros\", \"leagueoflegends\",\n",
        "              \"lgbt\", \"Libertarian\", \"linguistics\", \"MensRights\", \"news\", \"offbeat\", \"PoliticalCompassMemes\",\n",
        "              \"politics\",\n",
        "              \"teenagers\", \"TrueReddit\", \"TwoXChromosomes\", \"wallstreetbets\", \"worldnews\"]\n",
        "\n",
        "gathered_comments_file = open(\"gathered_comments.json\")\n",
        "gathered_comments = json.load(gathered_comments_file)\n",
        "\n",
        "num_comments = 3000\n",
        "\n",
        "selected_comments = []\n",
        "for subreddit in tqdm(subreddits):\n",
        "    number_of_comments = len(gathered_comments[subreddit])\n",
        "    if number_of_comments < num_comments:\n",
        "        selected_comments.extend(gathered_comments[subreddit])\n",
        "        continue\n",
        "    selected_comments.extend(random.sample(gathered_comments[subreddit], num_comments))\n",
        "\n",
        "print(\"Convert the list to a data frame\")\n",
        "# create a DataFrame from the selected comments\n",
        "df = pd.DataFrame(selected_comments)\n",
        "print(\"List converted\")\n",
        "\n",
        "# save the DataFrame to a CSV file\n",
        "df.to_csv('selected_comments.csv', index=False)\n",
        "print(\"file saved\")"
      ]
    }
  ]
}
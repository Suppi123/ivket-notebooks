{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ⚙️ Install Libraries and Download Dataset"
      ],
      "metadata": {
        "id": "8rZcp6fpNXc8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kkm3Qz_GNFWI"
      },
      "outputs": [],
      "source": [
        "!pip install openai zenodo-get pandas\n",
        "\n",
        "!zenodo_get 10.5281/zenodo.8023142"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✍️ Create Formal Comments\n",
        "\n",
        "Don't forget to enter your Open API Key in the variable API_KEY"
      ],
      "metadata": {
        "id": "T1G91IvlNieR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import pandas\n",
        "import statistics\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "\n",
        "API_KEY = \"ENTER_API_KEY_HERE\"\n",
        "\n",
        "# Change this sentence if you want to test other prompts\n",
        "style_change_sentence = \"Here is a rewrite of the text, which is more neutral:\"\n",
        "\n",
        "MIN_PERPLEXITY = 200\n",
        "number_of_comments_to_translate_per_subreddit = 150\n",
        "result_file_name = 'comments_to_neutral.json'\n",
        "\n",
        "data = pandas.io.parsers.read_csv(\"reddit_comments.csv\")\n",
        "data['perplexity'] = data.perplexity.astype(float)\n",
        "data = data.query(f\"perplexity > {MIN_PERPLEXITY}\")\n",
        "\n",
        "comment_lengths = []\n",
        "comment_candidates = []\n",
        "subreddits = data['subreddit']\n",
        "unique_subreddits = subreddits.unique()\n",
        "for subreddit in unique_subreddits:\n",
        "    subreddit_data = data.query(\"subreddit.str.contains('\" + subreddit + \"')\")\n",
        "    comments = []\n",
        "    for index, row in subreddit_data.iterrows():\n",
        "        comment_lengths.append(len(row.body))\n",
        "        comments.append({'body': row.body, 'subreddit': row.subreddit, 'perplexity': row.perplexity,\n",
        "                         'token_size': row.token_size})\n",
        "\n",
        "    filtered_comments = filter(lambda comment: comment['token_size'] >= 10, comments)\n",
        "    sorted_comments = sorted(filtered_comments, key=lambda x: x['perplexity'], reverse=True)\n",
        "    highest_perplexity_comments = sorted_comments[:number_of_comments_to_translate_per_subreddit]\n",
        "    comment_candidates.extend(highest_perplexity_comments)\n",
        "    # comment_candidates.extend(random.sample(comments, number_of_comments_to_translate_per_subreddit))\n",
        "\n",
        "\n",
        "print('Total comments: ' + str(len(data.index)))\n",
        "print('Filtered comments: ' + str(len(comment_candidates)))\n",
        "print('Median comment length of filtered comments: ' + str(statistics.median(comment_lengths)))\n",
        "\n",
        "prompts_to_send = []\n",
        "for comment in comment_candidates:\n",
        "    body = comment['body'].replace(\"\\\\n\", \"\")\n",
        "    body = re.sub(' +', ' ', body)\n",
        "    prompt = f\"Here is some text: {body} {style_change_sentence} {{\"\n",
        "    prompts_to_send.append({'prompt': prompt, 'body': body, 'subreddit': comment['subreddit']})\n",
        "\n",
        "\n",
        "def save_results(responses_to_save):\n",
        "    json_object = json.dumps({'data': responses_to_save}, indent=4)\n",
        "    with open(result_file_name, \"w\") as outfile:\n",
        "        outfile.write(json_object)\n",
        "\n",
        "\n",
        "# print(f\"About to translate {number_of_comments_to_translate} to a more formal style.\")\n",
        "responses = []\n",
        "openai.api_key = API_KEY\n",
        "for idx, prompt in enumerate(tqdm(prompts_to_send, desc=\"Translate comments to formal sentences\")):\n",
        "    response = \"\"\n",
        "    try:\n",
        "        response = openai.Completion.create(\n",
        "            model=\"text-davinci-003\",\n",
        "            prompt=prompt['prompt'],\n",
        "            temperature=0.2,\n",
        "            max_tokens=512,\n",
        "            top_p=1,\n",
        "            frequency_penalty=0.1,\n",
        "            presence_penalty=0\n",
        "        )\n",
        "    except Exception as e:\n",
        "        save_results(responses)\n",
        "        print(\"Exception occured (Previous results have been saved): \" + str(e))\n",
        "    responses.append({'response': response, 'body': prompt['body'], 'subreddit': prompt['subreddit']})\n",
        "\n",
        "save_results(responses)\n",
        "print(\"Work completed\")\n"
      ],
      "metadata": {
        "id": "mvj-8RivN8tX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
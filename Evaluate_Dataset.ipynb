{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Y5kzCJHj1s-O",
        "zND-pWMN3RJP",
        "Fgb2XWou3oZL",
        "CPdS5Mzk9NfM",
        "13kQmA8E8yNg"
      ],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ‚öôÔ∏è Install libraries and download dataset\n",
        "\n",
        "After this step, the runtime must be restarted."
      ],
      "metadata": {
        "id": "Y5kzCJHj1s-O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWHxnQH50-BL"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "!pip install simplet5 evaluate sacrebleu tqdm spacy matplotlib openai zenodo-get bert_score tensorflow\n",
        "\n",
        "!zenodo_get 10.5281/zenodo.8023142"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üèãÔ∏è Train evaluation models\n",
        "\n",
        "May take a while"
      ],
      "metadata": {
        "id": "Fgb2XWou3oZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas\n",
        "import datasets\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments, \\\n",
        "    AutoModelForCausalLM\n",
        "\n",
        "\n",
        "trainings_epochs = 5\n",
        "\n",
        "model_name = 'gpt2' # base-model for fine-tuning\n",
        "\n",
        "min_perplexity = 100 # minimal perplexity for trainings comments\n",
        "\n",
        "data_dir = 'trainings_data'\n",
        "\n",
        "use_submisson_data = False\n",
        "\n",
        "csv_comments_path = 'reddit_comments.csv'\n",
        "\n",
        "eval_models_dir = 'eval_models'\n",
        "\n",
        "trainings_data_dir = 'trainings_data'\n",
        "\n",
        "evaluation_models = [\"antiwork\", \"atheism\", \"Conservative\", \"conspiracy\", \"dankmemes\", \"gaybros\", \"leagueoflegends\",\n",
        "      \"lgbt\", \"Libertarian\", \"linguistics\", \"MensRights\", \"news\", \"offbeat\", \"PoliticalCompassMemes\", \"politics\",\n",
        "      \"teenagers\", \"TrueReddit\", \"TwoXChromosomes\", \"wallstreetbets\", \"worldnews\"]\n",
        "\n",
        "\n",
        "def create_trainings_data(subreddit_name, eval_split=0.8):\n",
        "    \"\"\"\n",
        "    Generates text files that are used for training the evaluation model. A training file with the naming\n",
        "    training_[subredditname].txt and an evaluation file with the naming eval_[subredditname].txt are created.\n",
        "    :param subreddit_name: Name of the subreddit for which an evaluation model is to be trained\n",
        "    :type subreddit_name: str\n",
        "    :param eval_split: Training split size. By default 0.8\n",
        "    :type eval_split float\n",
        "    :param data_dir: Directory in which the trainings data should be saved\n",
        "    :type data_dir: str\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "    comment_data = pandas.io.parsers.read_csv(csv_comments_path)\n",
        "    if use_submisson_data:\n",
        "        submission_data = pandas.io.parsers.read_csv(csv_comments_path)\n",
        "    query = '`subreddit` == ' + '\"' + subreddit_name + '\"'\n",
        "    comments_data_subreddit = comment_data.query(query)\n",
        "    if use_submisson_data:\n",
        "        submission_data_subreddit = submission_data.query(query)\n",
        "    number_of_comments = 1\n",
        "    for index, row in comments_data_subreddit.iterrows():\n",
        "        if row['body'] == \"[deleted]\":\n",
        "            continue\n",
        "        if row['perplexity'] < min_perplexity:\n",
        "            continue\n",
        "        number_of_comments += 1\n",
        "        texts.append(row['body'].strip() + ' <|endoftext|>')\n",
        "    if use_submisson_data:\n",
        "        for index, row in submission_data_subreddit.iterrows():\n",
        "            texts.append(row['title'].strip() + ' <|endoftext|>')\n",
        "    # split data in training and eval\n",
        "    split_index = int(len(texts) * eval_split)\n",
        "    trainings_data = texts[:split_index]\n",
        "    eval_data = texts[split_index:]\n",
        "    # write to file\n",
        "    training_text = \"\"\n",
        "    for sentence in trainings_data:\n",
        "        training_text += sentence + \"\\n\"\n",
        "    with open(f\"{data_dir}/training_{subreddit_name}.txt\", \"w\") as outfile:\n",
        "        outfile.write(training_text)\n",
        "    eval_text = \"\"\n",
        "    for sentence in eval_data:\n",
        "        eval_text += sentence + \"\\n\"\n",
        "    with open(f\"{data_dir}/eval_{subreddit_name}.txt\", \"w\") as outfile:\n",
        "        outfile.write(eval_text)\n",
        "    return number_of_comments\n",
        "\n",
        "\n",
        "def train_model(subreddit_name, model_dir=eval_models_dir):\n",
        "    \"\"\"\n",
        "    Train a model for the given subreddit\n",
        "    :param subreddit_name: Name of the subreddit for which an evaluation model is to be trained\n",
        "    :param data_dir: Folder in which the training data is located\n",
        "    :param model_dir: Folder in which the models should be saved\n",
        "    \"\"\"\n",
        "    train_path = f\"{data_dir}/training_{subreddit_name}.txt\"\n",
        "    eval_path = f\"{data_dir}/eval_{subreddit_name}.txt\"\n",
        "\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # currently only models that support the gpt2 tokenizer can be used\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "    output_dir = f\"./{model_dir}/{subreddit_name}\"\n",
        "    # create output folder if it not exists\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,  # The output directory\n",
        "        overwrite_output_dir=True,  # overwrite the content of the output directory\n",
        "        num_train_epochs=trainings_epochs,  # number of training epochs\n",
        "        per_device_train_batch_size=4,  # batch size for training\n",
        "        per_device_eval_batch_size=4,  # batch size for evaluation\n",
        "        eval_steps=400,  # Number of update steps between two evaluations.\n",
        "        save_steps=800,  # after # steps model is saved\n",
        "        warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
        "    )\n",
        "\n",
        "    dataset = datasets.load_dataset(\"text\", data_files={\"train\": train_path, \"test\": eval_path}, sample_by=\"line\")\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        # Remove empty lines\n",
        "        examples[\"text\"] = [line for line in examples[\"text\"] if len(line) > 0 and not line.isspace()]\n",
        "        return tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "        )\n",
        "\n",
        "    tokenized_dataset = dataset.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        num_proc=4,\n",
        "        remove_columns=[\"text\"],\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=tokenized_dataset[\"train\"],\n",
        "        eval_dataset=tokenized_dataset[\"test\"]\n",
        "    )\n",
        "    trainer.train()\n",
        "    trainer.save_model()\n",
        "\n",
        "\n",
        "# create folders that will be used later in the process\n",
        "if not os.path.exists(eval_models_dir):\n",
        "    os.makedirs(eval_models_dir)\n",
        "if not os.path.exists(trainings_data_dir):\n",
        "    os.makedirs(trainings_data_dir)\n",
        "\n",
        "# create the trainings and evaluation data for the evaluation models\n",
        "training_data_bar = tqdm(evaluation_models)\n",
        "data_stats = {}\n",
        "for subreddit in training_data_bar:\n",
        "    training_data_bar.set_description(f\"Processing {subreddit}\")\n",
        "    data_stats[subreddit] = create_trainings_data(subreddit_name=subreddit)\n",
        "print(f\"Data distribution {str(data_stats)}\")\n",
        "\n",
        "\n",
        "# train the evaluation models\n",
        "training_model_bar = tqdm(evaluation_models)\n",
        "for subreddit in tqdm(evaluation_models):\n",
        "    training_model_bar.set_description(f\"Training model for {subreddit}\")\n",
        "    train_model(subreddit_name=subreddit)\n"
      ],
      "metadata": {
        "id": "9tDfeikq3trK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üßÆ Calculate some small info about the dataset"
      ],
      "metadata": {
        "id": "eoL-2z4aPsds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "import pandas\n",
        "\n",
        "MIN_PERPLEXITY = 100\n",
        "\n",
        "data = pandas.io.parsers.read_csv(\"reddit_comments.csv\")\n",
        "data['perplexity'] = data.perplexity.astype(float)\n",
        "data = data.query(f\"perplexity > {MIN_PERPLEXITY}\")\n",
        "\n",
        "comments_lengths = []\n",
        "for index, row in data.iterrows():\n",
        "    comments_lengths.append(len(row.body))\n",
        "\n",
        "print('Median comment length of filtered comments: ' + str(statistics.median(comments_lengths)))\n",
        "\n",
        "data_grouped = data.groupby('subreddit')\n",
        "print(f\"Overall size: {len(data)}\")\n",
        "print(f\"Group sizes: {data_grouped.size()}\")"
      ],
      "metadata": {
        "id": "qgTkQDb2P4Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üë∑ Prepare the evaluation"
      ],
      "metadata": {
        "id": "CPdS5Mzk9NfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "def get_perplexity(model, encodings):\n",
        "    max_length = model.config.n_positions\n",
        "    stride = 512\n",
        "    seq_len = encodings.input_ids.size(1)\n",
        "\n",
        "    nlls = []\n",
        "    prev_end_loc = 0\n",
        "    for begin_loc in range(0, seq_len, stride):\n",
        "        end_loc = min(begin_loc + max_length, seq_len)\n",
        "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
        "        input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
        "        target_ids = input_ids.clone()\n",
        "        target_ids[:, :-trg_len] = -100\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, labels=target_ids)\n",
        "\n",
        "            # loss is calculated using CrossEntropyLoss which averages over input tokens.\n",
        "            # Multiply it with trg_len to get the summation instead of average.\n",
        "            # We will take average over all the tokens to get the true average\n",
        "            # in the last step of this example.\n",
        "            neg_log_likelihood = outputs.loss * trg_len\n",
        "\n",
        "        nlls.append(neg_log_likelihood)\n",
        "\n",
        "        prev_end_loc = end_loc\n",
        "        if end_loc == seq_len:\n",
        "            break\n",
        "\n",
        "    return torch.exp(torch.stack(nlls).sum() / end_loc).item()\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    # check if cuda is available and set device\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    return torch.device('cpu')\n",
        "\n",
        "\n",
        "class StyleSpecificPerplexity:\n",
        "\n",
        "    def __init__(self):\n",
        "        # load config\n",
        "        self.config = {\"evaluation_models\": [\"antiwork\", \"atheism\",\n",
        "                       \"Conservative\", \"conspiracy\", \"dankmemes\", \"gaybros\",\n",
        "                       \"leagueoflegends\", \"lgbt\", \"Libertarian\", \"linguistics\",\n",
        "                       \"MensRights\", \"news\", \"offbeat\", \"PoliticalCompassMemes\",\n",
        "                       \"politics\", \"teenagers\", \"TrueReddit\", \"TwoXChromosomes\",\n",
        "                       \"wallstreetbets\", \"worldnews\"],\n",
        "                       \"eval_model_dir\": 'eval_models'}\n",
        "        self.models = {}\n",
        "        for model_name in self.config[\"evaluation_models\"]:\n",
        "            device = get_device()\n",
        "            # load tokenizer an model\n",
        "            model = AutoModelForCausalLM.from_pretrained(f\"{self.config['eval_model_dir']}/{model_name}\").to(device)\n",
        "            tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "            self.models[model_name] = {\"model\": model, \"tokenizer\": tokenizer}\n",
        "\n",
        "    def calculate_perplexity(self, model_name, input_texts=[]):\n",
        "        if model_name not in self.models:\n",
        "            raise Exception(f\"No model for style {model_name}\")\n",
        "\n",
        "        # load model and tokenizer\n",
        "        model = self.models[model_name][\"model\"]\n",
        "        tokenizer = self.models[model_name][\"tokenizer\"]\n",
        "\n",
        "        perplexities = []\n",
        "        for input_text in input_texts:\n",
        "            # encode text\n",
        "            device = get_device()\n",
        "            encodings = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "            # calculate perplexity\n",
        "            perplexity = get_perplexity(model, encodings)\n",
        "\n",
        "            # If no perplexity could be calculated (for example because the input contains only one word),\n",
        "            # do not save. Result would be NaN. This means that no more total perplexity can be calculated.\n",
        "            if math.isnan(perplexity):\n",
        "                continue\n",
        "\n",
        "            perplexities.append(perplexity)\n",
        "        # return perplexity values\n",
        "        return perplexities\n"
      ],
      "metadata": {
        "id": "oo8Ntbbk9XY-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚öóÔ∏è Evaluate dataset lemma overlap and perplexity"
      ],
      "metadata": {
        "id": "13kQmA8E8yNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import multiprocessing\n",
        "\n",
        "import pandas\n",
        "import spacy\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "min_perplexity = 100 # minimal perplexity for trainings comments\n",
        "\n",
        "data_dir = 'trainings_data'\n",
        "\n",
        "use_submisson_data = False\n",
        "\n",
        "csv_comments_path = 'reddit_comments.csv'\n",
        "\n",
        "eval_models_dir = 'eval_models'\n",
        "\n",
        "result_path = 'dataset_eval_results'\n",
        "\n",
        "trainings_data_dir = 'trainings_data'\n",
        "\n",
        "evaluation_models = [\"antiwork\", \"atheism\", \"Conservative\", \"conspiracy\", \"dankmemes\", \"gaybros\", \"leagueoflegends\",\n",
        "      \"lgbt\", \"Libertarian\", \"linguistics\", \"MensRights\", \"news\", \"offbeat\", \"PoliticalCompassMemes\", \"politics\",\n",
        "      \"teenagers\", \"TrueReddit\", \"TwoXChromosomes\", \"wallstreetbets\", \"worldnews\"]\n",
        "\n",
        "if not os.path.exists(result_path):\n",
        "  os.makedirs(result_path)\n",
        "\n",
        "data = pandas.io.parsers.read_csv(csv_comments_path)\n",
        "data['perplexity'] = data.perplexity.astype(float)\n",
        "\n",
        "data = data.query(f\"perplexity > {float(min_perplexity)}\")\n",
        "# for n-grams, default are bigrams\n",
        "n = 2\n",
        "\n",
        "calculate_overlap = True\n",
        "calculate_perplexities = True\n",
        "\n",
        "# Calculate overlapping tokens between the subreddits\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "def preprocess(text: str):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
        "    ngrams = []\n",
        "    for i in range(len(doc) - n + 1):\n",
        "        ngram = \" \".join([token.text for token in doc[i:i + n]])\n",
        "        ngrams.append(ngram)\n",
        "    return {'tokens': tokens, 'ngrams': ngrams}\n",
        "\n",
        "\n",
        "def preprocess_parallel(data_chunk):\n",
        "    data_chunk['result'] = data_chunk['body'].apply(preprocess)\n",
        "    return data_chunk\n",
        "\n",
        "\n",
        "if calculate_overlap:\n",
        "    # Split the data into chunks\n",
        "    num_processes = multiprocessing.cpu_count()\n",
        "    chunks = np.array_split(data, num_processes)\n",
        "\n",
        "    # Create a pool of workers to execute the preprocess function in parallel\n",
        "    pool = multiprocessing.Pool(processes=num_processes)\n",
        "\n",
        "    # Preprocess the data chunks in parallel\n",
        "    processed_chunks = pool.map(preprocess_parallel, chunks)\n",
        "\n",
        "    # Concatenate the processed chunks back into a single dataframe\n",
        "    processed_data = pandas.concat(processed_chunks)\n",
        "\n",
        "    # Group the processed data by subreddit\n",
        "    subreddit_result = processed_data.groupby('subreddit')['result'].apply(list).to_dict()\n",
        "\n",
        "    for subreddit1, results1 in tqdm(subreddit_result.items()):\n",
        "        flat_tokens1 = []\n",
        "        flat_ngrams1 = []\n",
        "        for res in results1:\n",
        "            # print(res)\n",
        "            flat_tokens1.extend(res['tokens'])\n",
        "            flat_ngrams1.extend(res['ngrams'])\n",
        "\n",
        "        # print(flat_tokens1)\n",
        "\n",
        "        unique_tokens1 = set(flat_tokens1)\n",
        "        unique_ngrams1 = set(flat_ngrams1)\n",
        "        for subreddit2, results2 in subreddit_result.items():\n",
        "            flat_tokens2 = []\n",
        "            flat_ngrams2 = []\n",
        "            for res in results2:\n",
        "                flat_tokens2.extend(res['tokens'])\n",
        "                flat_ngrams2.extend(res['ngrams'])\n",
        "\n",
        "            unique_tokens2 = set(flat_tokens2)\n",
        "            unique_ngrams2 = set(flat_ngrams2)\n",
        "            # Create filename\n",
        "            filename = f\"{result_path}/{subreddit1}_{subreddit2}_overlap.json\"\n",
        "\n",
        "            # Check if file already exists -> skip calculation if file already exists\n",
        "            if os.path.isfile(filename):\n",
        "                print(f\"Overlap for {subreddit1} {subreddit2} already exits. Skip this combination.\")\n",
        "                continue\n",
        "\n",
        "            # Calculate overlap\n",
        "            overlap_tokens = unique_tokens1.intersection(unique_tokens2)\n",
        "            overlap_ngrams = unique_ngrams1.intersection(unique_ngrams2)\n",
        "\n",
        "            result_object = {'subreddit1': subreddit1, 'subreddit2': subreddit2,\n",
        "                             'overlap_tokens': len(overlap_tokens),\n",
        "                             'overlap_ngrams': len(overlap_ngrams),\n",
        "                             'subreddit1_total_tokens': len(unique_tokens1),\n",
        "                             'subreddit2_total_tokens': len(unique_tokens2),\n",
        "                             'subreddit1_total_ngrams': len(unique_ngrams1),\n",
        "                             'subreddit2_total_ngrams': len(unique_ngrams2)}\n",
        "\n",
        "            # Save the result\n",
        "            with open(filename, \"w\") as r:\n",
        "                json.dump(result_object, r)\n",
        "\n",
        "\n",
        "# calculate the perplexity of models trained on subreddits against content from other subreddits\n",
        "style_specific_perplexity = StyleSpecificPerplexity()\n",
        "\n",
        "if calculate_perplexities:\n",
        "    # Define a function to calculate perplexity\n",
        "    def calculate_perplexity(model_name, subreddit_data):\n",
        "        perplexity_subreddit = style_specific_perplexity.calculate_perplexity(model_name, subreddit_data)\n",
        "        return perplexity_subreddit\n",
        "\n",
        "\n",
        "    # Loop through each subreddit\n",
        "    for subreddit in tqdm(data['subreddit'].unique()):\n",
        "        # for subreddit in tqdm(config['evaluation_models']):\n",
        "        # Get the comments for the current subreddit\n",
        "        comments = data[data['subreddit'] == subreddit]['body'].tolist()\n",
        "\n",
        "        # Loop through each other subreddit\n",
        "        for other_subreddit in data['subreddit'].unique():\n",
        "\n",
        "            if subreddit in evaluation_models:\n",
        "\n",
        "                # Create filename\n",
        "                filename = f\"{result_path}/{subreddit}_{other_subreddit}_perplexity.json\"\n",
        "\n",
        "                # Check if file already exists -> skip calculation if file already exists\n",
        "                if os.path.isfile(filename):\n",
        "                    print(f\"Perplexity for {subreddit} {other_subreddit} already exits. Skip this combination.\")\n",
        "                    continue\n",
        "\n",
        "                # Get the comments for the other subreddit\n",
        "                other_comments = data[data['subreddit'] == other_subreddit]['body'].tolist()\n",
        "\n",
        "                # Calculate perplexity for the current subreddit using the model\n",
        "                perplexities = calculate_perplexity(subreddit, other_comments)\n",
        "\n",
        "                # Store the results in a dictionary\n",
        "                result_object = {'subreddit1': subreddit, 'subreddit2': other_subreddit, 'perplexities': perplexities}\n",
        "\n",
        "                # Save the result\n",
        "                with open(filename, \"w\") as r:\n",
        "                    json.dump(result_object, r)\n"
      ],
      "metadata": {
        "id": "1egURm3G9Drt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ Evaluate formal translations"
      ],
      "metadata": {
        "id": "jNSBvBId-EUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "trainings_data_path = 'training_labeled_with_style_samples.json'\n",
        "eval_data_path = 'eval_labeled_with_style_samples.json'\n",
        "\n",
        "# load models\n",
        "bert_score_model = evaluate.load(\"bertscore\")\n",
        "perplexity_model = evaluate.load(\"perplexity\", module_type=\"measurement\")\n",
        "bleu_model = evaluate.load(\"chrf\")\n",
        "\n",
        "# merge eval and training file for evaluation\n",
        "def merge_json_files(file1, file2):\n",
        "    with open(file1, 'r') as f1, open(file2, 'r') as f2:\n",
        "        data1 = json.load(f1)\n",
        "        data2 = json.load(f2)\n",
        "\n",
        "    merged_data = {}\n",
        "    merged_data['data'] = []\n",
        "    merged_data['data'].extend(data1['data'])\n",
        "    merged_data['data'].extend(data2['data'])\n",
        "    return merged_data\n",
        "\n",
        "tmp_scores = {}\n",
        "####################################################\n",
        "# Create scores for the GYAFC-Corpus as baseline   #\n",
        "####################################################\n",
        "\n",
        "# We cannot calculate the metrics for the GYAFC dataset\n",
        "# because it is not publicly available\n",
        "\n",
        "####################################################\n",
        "#         Create scores for the dataset            #\n",
        "####################################################\n",
        "\n",
        "# load prepared data\n",
        "trainings_data_json = merge_json_files(trainings_data_path,\n",
        "                                                     eval_data_path)\n",
        "\n",
        "references = []\n",
        "predictions = []\n",
        "for trainings_object in trainings_data_json[\"data\"]:\n",
        "    if len(trainings_object['input_sentence']) == 0:\n",
        "        continue\n",
        "    references.append([trainings_object['result_sentence']])\n",
        "    predictions.append(trainings_object['input_sentence'])\n",
        "\n",
        "# create a flat list of references, needed for the calculation of BERTScore\n",
        "flat_references = [item for sublist in references for item in sublist]\n",
        "\n",
        "print(\"About to calculate dataset label scores\")\n",
        "\n",
        "# calculate scores\n",
        "tmp_scores['Dataset'] = {}\n",
        "tmp_scores['Dataset']['bert_score'] = bert_score_model.compute(predictions=predictions, references=flat_references,\n",
        "                                                               lang=\"en\", model_type=\"microsoft/deberta-xlarge-mnli\")\n",
        "tmp_scores['Dataset']['bleu_score'] = bleu_model.compute(predictions=predictions, references=references, word_order=2)\n",
        "tmp_scores['Dataset']['perplexity_score'] = perplexity_model.compute(data=predictions, model_id='gpt2')\n",
        "\n",
        "print(\"Dataset label scores calculated\")\n",
        "\n",
        "result_obj = {}\n",
        "# create result object\n",
        "for dataset in ['Dataset']:\n",
        "    # BERT\n",
        "    bert_scores = tmp_scores[dataset]['bert_score']\n",
        "    bert_f1_mean_score = sum(bert_scores['f1']) / len(bert_scores['f1'])\n",
        "    bert_precision_mean_score = sum(bert_scores['precision']) / len(bert_scores['precision'])\n",
        "    bert_recall_mean_score = sum(bert_scores['recall']) / len(bert_scores['recall'])\n",
        "\n",
        "    bert_score = {'mean_f1': bert_f1_mean_score, 'mean_precision': bert_precision_mean_score,\n",
        "                  'mean_recall': bert_recall_mean_score}\n",
        "    # Perplexity\n",
        "    perplexity_score = tmp_scores[dataset]['perplexity_score']\n",
        "    median_perplexity = np.median(perplexity_score['perplexities'])\n",
        "    variance_perplexity = np.var(perplexity_score['perplexities'])\n",
        "    perplexity = {'perplexity_median': median_perplexity, 'perplexity_variance': variance_perplexity,\n",
        "                  'perplexity_mean': perplexity_score['mean_perplexity']}\n",
        "    # save results in object\n",
        "    result_obj[dataset] = {'BLEU': tmp_scores[dataset]['bleu_score'], 'Perplexity': perplexity,\n",
        "                           'BERTScore': bert_score}\n",
        "\n",
        "# save result object\n",
        "results_file = \"dataset_label_eval_result.json\"\n",
        "with open(results_file, \"w\") as r:\n",
        "    print(f\"Saved Eval Results in: {results_file}\")\n",
        "    json.dump(result_obj, r)\n"
      ],
      "metadata": {
        "id": "CX7RxeSN-LJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Create charts"
      ],
      "metadata": {
        "id": "qvB04fCiEW8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the directory where the data files are stored\n",
        "data_dir = 'dataset_eval_results'\n",
        "\n",
        "# Get a list of all the subreddit names\n",
        "subreddits = set()\n",
        "for filename in os.listdir(data_dir):\n",
        "    # Check if the file is a JSON file\n",
        "    if filename.endswith('perplexity.json'):\n",
        "        # Load the JSON data from the file\n",
        "        with open(os.path.join(data_dir, filename), 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Get the subreddit names from the data\n",
        "        subreddit1 = data['subreddit1']\n",
        "        subreddit2 = data['subreddit2']\n",
        "        subreddits.add(subreddit1)\n",
        "        subreddits.add(subreddit2)\n",
        "\n",
        "# Sort the list of subreddit names alphabetically\n",
        "subreddits = sorted(list(subreddits))\n",
        "\n",
        "# Create an empty matrix to store the median perplexity values\n",
        "matrix = np.zeros((len(subreddits), len(subreddits)))\n",
        "\n",
        "# Loop through all the files in the directory\n",
        "for filename in os.listdir(data_dir):\n",
        "    # Check if the file is a JSON file\n",
        "    if filename.endswith('perplexity.json'):\n",
        "        # Load the JSON data from the file\n",
        "        with open(os.path.join(data_dir, filename), 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Get the subreddit names and perplexities from the data\n",
        "        subreddit1 = data['subreddit1']\n",
        "        subreddit2 = data['subreddit2']\n",
        "        perplexities = np.array(data['perplexities'])\n",
        "\n",
        "        # Calculate the median perplexity score\n",
        "        median_perplexity = np.median(perplexities)\n",
        "\n",
        "        # Add the median perplexity to the matrix\n",
        "        i = subreddits.index(subreddit1)\n",
        "        j = subreddits.index(subreddit2)\n",
        "        matrix[i, j] = median_perplexity\n",
        "        # matrix[j, i] = median_perplexity\n",
        "\n",
        "# Create the matrix plot\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "im = ax.imshow(matrix, cmap='viridis')\n",
        "\n",
        "# Set the tick labels\n",
        "ax.set_xticks(np.arange(len(subreddits)))\n",
        "ax.set_yticks(np.arange(len(subreddits)))\n",
        "ax.set_xticklabels(subreddits, rotation=90)\n",
        "ax.set_yticklabels(subreddits)\n",
        "\n",
        "# Set the axis labels\n",
        "ax.set_xlabel('Subreddit')\n",
        "ax.set_ylabel('Subreddit')\n",
        "\n",
        "# Add the colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "cbar.ax.set_ylabel('Median Perplexity', rotation=-90, va='bottom')\n",
        "\n",
        "# Add the title\n",
        "ax.set_title('Median Perplexity Matrix')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "##############################################################\n",
        "\n",
        "# Get a list of all the subreddit names\n",
        "subreddits = set()\n",
        "for filename in os.listdir(data_dir):\n",
        "    # Check if the file is a JSON file\n",
        "    if filename.endswith('overlap.json'):\n",
        "        # Load the JSON data from the file\n",
        "        with open(os.path.join(data_dir, filename), 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Get the subreddit names from the data\n",
        "        subreddit1 = data['subreddit1']\n",
        "        subreddit2 = data['subreddit2']\n",
        "        subreddits.add(subreddit1)\n",
        "        subreddits.add(subreddit2)\n",
        "\n",
        "# Sort the list of subreddit names alphabetically\n",
        "subreddits = sorted(list(subreddits))\n",
        "\n",
        "# Create an empty matrix to store the median perplexity values\n",
        "matrix = np.zeros((len(subreddits), len(subreddits)))\n",
        "\n",
        "# Loop through all the files in the directory\n",
        "for filename in os.listdir(data_dir):\n",
        "    # Check if the file is a JSON file\n",
        "    if filename.endswith('overlap.json'):\n",
        "        # Load the JSON data from the file\n",
        "        with open(os.path.join(data_dir, filename), 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Get the subreddit names and overlap from the data\n",
        "        subreddit1 = data['subreddit1']\n",
        "        subreddit2 = data['subreddit2']\n",
        "        subreddit1_total_tokens = data['subreddit1_total_tokens']\n",
        "        subreddit2_total_tokens = data['subreddit2_total_tokens']\n",
        "        overlap = data['overlap_tokens']\n",
        "\n",
        "        total_tokens = subreddit1_total_tokens + subreddit2_total_tokens\n",
        "        overlap_percent = (overlap / total_tokens) * 100\n",
        "\n",
        "        # Add the median perplexity to the matrix\n",
        "        i = subreddits.index(subreddit1)\n",
        "        j = subreddits.index(subreddit2)\n",
        "        matrix[i, j] = overlap_percent\n",
        "        # matrix[j, i] = median_perplexity\n",
        "\n",
        "# Create the matrix plot\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "im = ax.imshow(matrix, cmap='viridis')\n",
        "\n",
        "# Set the tick labels\n",
        "ax.set_xticks(np.arange(len(subreddits)))\n",
        "ax.set_yticks(np.arange(len(subreddits)))\n",
        "ax.set_xticklabels(subreddits, rotation=90)\n",
        "ax.set_yticklabels(subreddits)\n",
        "\n",
        "# Set the axis labels\n",
        "ax.set_xlabel('Subreddit')\n",
        "ax.set_ylabel('Subreddit')\n",
        "\n",
        "# Add the colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "cbar.ax.set_ylabel('Overlapping Tokens (in percent)', rotation=-90, va='bottom')\n",
        "\n",
        "# Add the title\n",
        "ax.set_title('Overlap Matrix')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "##############################################################\n",
        "\n",
        "# Get a list of all the subreddit names\n",
        "subreddits = set()\n",
        "for filename in os.listdir(data_dir):\n",
        "    # Check if the file is a JSON file\n",
        "    if filename.endswith('overlap.json'):\n",
        "        # Load the JSON data from the file\n",
        "        with open(os.path.join(data_dir, filename), 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Get the subreddit names from the data\n",
        "        subreddit1 = data['subreddit1']\n",
        "        subreddit2 = data['subreddit2']\n",
        "        subreddits.add(subreddit1)\n",
        "        subreddits.add(subreddit2)\n",
        "\n",
        "# Sort the list of subreddit names alphabetically\n",
        "subreddits = sorted(list(subreddits))\n",
        "\n",
        "# Create an empty matrix to store the median perplexity values\n",
        "matrix = np.zeros((len(subreddits), len(subreddits)))\n",
        "\n",
        "# Loop through all the files in the directory\n",
        "for filename in os.listdir(data_dir):\n",
        "    # Check if the file is a JSON file\n",
        "    if filename.endswith('overlap.json'):\n",
        "        # Load the JSON data from the file\n",
        "        with open(os.path.join(data_dir, filename), 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Get the subreddit names and overlap from the data\n",
        "        subreddit1 = data['subreddit1']\n",
        "        subreddit2 = data['subreddit2']\n",
        "        total_ngrams_subreddit1 = data['subreddit1_total_ngrams']\n",
        "        total_ngrams_subreddit2 = data['subreddit2_total_ngrams']\n",
        "        overlap = data['overlap_ngrams']\n",
        "\n",
        "        total_ngrams = total_ngrams_subreddit1 + total_ngrams_subreddit2\n",
        "        overlap_percent = (overlap / total_ngrams) * 100\n",
        "\n",
        "        # Add the median perplexity to the matrix\n",
        "        i = subreddits.index(subreddit1)\n",
        "        j = subreddits.index(subreddit2)\n",
        "        matrix[i, j] = overlap_percent\n",
        "        # matrix[j, i] = median_perplexity\n",
        "\n",
        "# Create the matrix plot\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "im = ax.imshow(matrix, cmap='viridis')\n",
        "\n",
        "# Set the tick labels\n",
        "ax.set_xticks(np.arange(len(subreddits)))\n",
        "ax.set_yticks(np.arange(len(subreddits)))\n",
        "ax.set_xticklabels(subreddits, rotation=90)\n",
        "ax.set_yticklabels(subreddits)\n",
        "\n",
        "# Set the axis labels\n",
        "ax.set_xlabel('Subreddit')\n",
        "ax.set_ylabel('Subreddit')\n",
        "\n",
        "# Add the colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "cbar.ax.set_ylabel('Overlapping Bigrams (in percent)', rotation=-90, va='bottom')\n",
        "\n",
        "# Add the title\n",
        "ax.set_title('Overlap Matrix')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QiRYZMzgEd62"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
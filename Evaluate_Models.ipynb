{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "collapsed_sections": [
        "NZlGwBgtcC_l",
        "6vDMczy_dInU",
        "CceHKPBfdNtY",
        "4m-Ki7hk4_42",
        "Za5r_apg7MiY",
        "ReE4iP9NBwEx",
        "1GBW-_2vFoNv",
        "YLHwKTPPDDwb",
        "uBzJk_RpCeUt",
        "xRYGSgWF6kPK"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ‚öôÔ∏è Install Libraries and Download Dataset\n",
        "\n",
        "After this step, the runtime must be restarted."
      ],
      "metadata": {
        "id": "NZlGwBgtcC_l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7T_hJLWWB7F"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "!pip install simplet5 evaluate sacrebleu tqdm spacy matplotlib openai zenodo-get bert_score tensorflow\n",
        "\n",
        "!zenodo_get 10.5281/zenodo.8023142"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìù Initialize config\n",
        "Here you can also determine which models should be evaluated. By default, models are used that were trained during the master thesis.\n",
        "\n",
        "To change the models use the x-model property. Here you can also specify local models by specifying the appropriate file path."
      ],
      "metadata": {
        "id": "6vDMczy_dInU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"models\": [\"t5-small\", \"t5-base\", \"t5-large\"],\n",
        "    \"result_path\": \"./dataset_eval_results/\",\n",
        "    \"csv_comments_path\": \"../data/filtered_data/intense_style_comments.csv\",\n",
        "    \"gyafc_path\": [\"../data/GYAFC/GYAFC_Corpus/Entertainment_Music/train\",\n",
        "      \"../data/GYAFC/GYAFC_Corpus/Family_Relationships/train\"],\n",
        "    \"t5_models\": [{\"name\": \"t5-base\", \"path\": \"Suppi123/T5-Base-Text-Style-Transfer-Using-Examples\"},\n",
        "                  {\"name\": \"flan-t5-base\", \"path\": \"Suppi123/Flan-T5-Base-Text-Style-Transfer-Using-Examples\"}],\n",
        "    \"bert_models\": [\"Suppi123/Bert-Base-Uncased-Text-Style-Transfer-Using-Examples\"],\n",
        "    \"bart_models\": [{\"name\": \"bart-base\", \"path\": \"Suppi123/Bart-Base-Text-Style-Transfer-Using-Examples\"}],\n",
        "    \"gptneo_models\": [\"Suppi123/GPT-NEO-2.7B-Text-Style-Transfer-Using-Examples\"],\n",
        "    \"trainings_data_path\": \"training_labeled_with_style_samples.json\",\n",
        "    \"eval_data_path\": \"eval_labeled_with_style_samples.json\",\n",
        "    \"min_perplexity\": 100,\n",
        "    \"result_data\": \"results/bart_epoch0_results.json\",\n",
        "    \"evaluation_models\": [\"antiwork\", \"atheism\", \"Conservative\", \"conspiracy\", \"dankmemes\", \"gaybros\", \"leagueoflegends\",\n",
        "      \"lgbt\", \"Libertarian\", \"linguistics\", \"MensRights\", \"news\", \"offbeat\", \"PoliticalCompassMemes\", \"politics\",\n",
        "      \"teenagers\", \"TrueReddit\", \"TwoXChromosomes\", \"wallstreetbets\", \"worldnews\"]\n",
        "}"
      ],
      "metadata": {
        "id": "DoKB97k5cV0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úçÔ∏è Generate output with T5 and Flan-T5"
      ],
      "metadata": {
        "id": "CceHKPBfdNtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from simplet5 import SimpleT5\n",
        "\n",
        "\n",
        "# Load prompts\n",
        "eval_data_file = open(config['eval_data_path'])\n",
        "eval_data = json.load(eval_data_file)['data']\n",
        "\n",
        "\n",
        "input_texts = []\n",
        "for prompt in eval_data:\n",
        "    trainings_prompt = 'Here a example sentences: '\n",
        "    for sample in prompt['style_samples']:\n",
        "        trainings_prompt += '{' + sample + '} '\n",
        "    trainings_prompt += 'Here is a sentence: {' + prompt['input_sentence'] + '} '\n",
        "    trainings_prompt += 'Here is a rewrite of this sentence according to the example sentences: {'\n",
        "    input_texts.append({'prompt': trainings_prompt, 'input': prompt['input_sentence'],\n",
        "                        'subreddit': prompt['subreddit'], 'result_sentence': prompt['result_sentence']})\n",
        "\n",
        "\n",
        "for model_config in config['t5_models']:\n",
        "    results = []\n",
        "    model = SimpleT5()\n",
        "    model.load_model(\"t5\", f\"{model_config['path']}\", use_gpu=True)\n",
        "    for input_text in input_texts:\n",
        "        output_text = model.predict(source_text=input_text['prompt'], repetition_penalty=1.0,\n",
        "                                    num_return_sequences=1, num_beams=5)\n",
        "        results.append({'prompt': input_text['prompt'], 'input': input_text['input'], 'output': output_text[0],\n",
        "                        'subreddit': input_text['subreddit'], 'result_sentence': input_text['result_sentence']})\n",
        "\n",
        "    json_result_object = json.dumps({'data': results}, indent=4)\n",
        "    with open(f\"{model_config['name']}_results.json\", \"w\") as outfile:\n",
        "        outfile.write(json_result_object)\n"
      ],
      "metadata": {
        "id": "lw3Ro5oVcTAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úçÔ∏è Generate output with BART"
      ],
      "metadata": {
        "id": "4m-Ki7hk4_42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "# Load prompts\n",
        "eval_data_file = open(config['eval_data_path'])\n",
        "eval_data = json.load(eval_data_file)['data']\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
        "\n",
        "sep_token = tokenizer.sep_token\n",
        "bos_token = tokenizer.bos_token\n",
        "eos_token = tokenizer.eos_token\n",
        "\n",
        "input_texts = []\n",
        "for prompt in eval_data:\n",
        "    trainings_prompt = \"\"\n",
        "    for sample in prompt['style_samples']:\n",
        "        trainings_prompt += bos_token + sample + sep_token\n",
        "    trainings_prompt += bos_token + prompt['input_sentence'] + eos_token\n",
        "    input_texts.append({'prompt': trainings_prompt, 'input': prompt['input_sentence'],\n",
        "                        'subreddit': prompt['subreddit']})\n",
        "\n",
        "test_models = tqdm(config['bart_models'])\n",
        "for model_config in test_models:\n",
        "    model_path = model_config['path']\n",
        "    model_name = model_config['name']\n",
        "    results = []\n",
        "    test_models.set_description(f\"Processing {model_name}\")\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_path).to(device)\n",
        "    for input_text in input_texts:\n",
        "        input_ids = tokenizer.encode(input_text['prompt'], return_tensors='pt').to(device)\n",
        "        output_ids = model.generate(input_ids, max_length=512, num_beams=5, early_stopping=True)\n",
        "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        results.append({'prompt': input_text['prompt'], 'input': input_text['input'], 'output': output_text,\n",
        "                        'subreddit': input_text['subreddit']})\n",
        "\n",
        "    json_result_object = json.dumps({'data': results}, indent=4)\n",
        "    with open(f\"{model_name}_results.json\", \"w\") as outfile:\n",
        "        outfile.write(json_result_object)\n"
      ],
      "metadata": {
        "id": "2TOjvJs65HM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úçÔ∏è Generate output with BERT"
      ],
      "metadata": {
        "id": "Za5r_apg7MiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import BertLMHeadModel, BertTokenizer\n",
        "\n",
        "# Load prompts\n",
        "eval_data_file = open(config['eval_data_path'])\n",
        "eval_data = json.load(eval_data_file)['data']\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "sep_token = tokenizer.sep_token\n",
        "\n",
        "input_texts = []\n",
        "for prompt in eval_data:\n",
        "    trainings_prompt = \"\"\n",
        "    for sample in prompt['style_samples']:\n",
        "        trainings_prompt += sep_token + sample + sep_token\n",
        "    trainings_prompt += sep_token + prompt['input_sentence'] + sep_token\n",
        "    input_texts.append({'prompt': trainings_prompt, 'input': prompt['input_sentence'],\n",
        "                        'subreddit': prompt['subreddit']})\n",
        "\n",
        "test_models = tqdm(config['bert_models'])\n",
        "for model_name in test_models:\n",
        "    results = []\n",
        "    test_models.set_description(f\"Processing {model_name}\")\n",
        "    model = BertLMHeadModel.from_pretrained(model_name).to(device)\n",
        "    for input_text in input_texts:\n",
        "        input_ids = tokenizer.encode(input_text['prompt'], return_tensors='pt').to(device)\n",
        "        output_ids = model.generate(input_ids, max_length=512, num_beams=5, early_stopping=True)\n",
        "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        print(output_text)\n",
        "        results.append({'prompt': input_text['prompt'], 'input': input_text['input'], 'output': output_text,\n",
        "                        'subreddit': input_text['subreddit']})\n",
        "\n",
        "    json_result_object = json.dumps({'data': results}, indent=4)\n",
        "    with open(f\"{model_name}_results.json\", \"w\") as outfile:\n",
        "        outfile.write(json_result_object)\n"
      ],
      "metadata": {
        "id": "FulCqWIz7OzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úçÔ∏è Generate output with GPT-Neo"
      ],
      "metadata": {
        "id": "ReE4iP9NBwEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
        "\n",
        "\n",
        "# Load prompts\n",
        "eval_data_file = open(config['eval_data_path'])\n",
        "eval_data = json.load(eval_data_file)['data']\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-2.7B')\n",
        "\n",
        "sep_token = tokenizer.eos_token\n",
        "\n",
        "input_texts = []\n",
        "for prompt in eval_data:\n",
        "    training_prompt = \"\"\n",
        "    for sample in prompt['style_samples']:\n",
        "        training_prompt += sep_token + sample + sep_token\n",
        "    training_prompt += sep_token + prompt['input_sentence'] + sep_token\n",
        "    input_texts.append({'prompt': training_prompt, 'input': prompt['input_sentence'],\n",
        "                        'subreddit': prompt['subreddit']})\n",
        "\n",
        "test_models = tqdm(config['gptneo_models'])\n",
        "for model_name in test_models:\n",
        "    results = []\n",
        "    test_models.set_description(f\"Processing {model_name}\")\n",
        "    model = GPTNeoForCausalLM.from_pretrained(model_name).to(device)\n",
        "    for input_text in input_texts:\n",
        "        input_ids = tokenizer.encode(input_text['prompt'], return_tensors='pt').to(device)\n",
        "        output_ids = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
        "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        print(output_text)\n",
        "        results.append({'prompt': input_text['prompt'], 'input': input_text['input'], 'output': output_text,\n",
        "                        'subreddit': input_text['subreddit']})\n",
        "\n",
        "    json_result_object = json.dumps({'data': results}, indent=4)\n",
        "    with open(f\"{model_name}_results.json\", \"w\") as outfile:\n",
        "        outfile.write(json_result_object)\n"
      ],
      "metadata": {
        "id": "3M_MQHl2B16D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üèãÔ∏è Train evaluation models\n",
        "\n",
        "Optional. You only need it if you want to examine the style-specific perplexities of the data.\n",
        "\n",
        "May take a while"
      ],
      "metadata": {
        "id": "1GBW-_2vFoNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas\n",
        "import datasets\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments, \\\n",
        "    AutoModelForCausalLM\n",
        "\n",
        "\n",
        "trainings_epochs = 5\n",
        "\n",
        "model_name = 'gpt2' # base-model for fine-tuning\n",
        "\n",
        "min_perplexity = 100 # minimal perplexity for trainings comments\n",
        "\n",
        "data_dir = 'trainings_data'\n",
        "\n",
        "use_submisson_data = False\n",
        "\n",
        "csv_comments_path = 'reddit_comments.csv'\n",
        "\n",
        "eval_models_dir = 'eval_models'\n",
        "\n",
        "trainings_data_dir = 'trainings_data'\n",
        "\n",
        "evaluation_models = [\"antiwork\", \"atheism\", \"Conservative\", \"conspiracy\", \"dankmemes\", \"gaybros\", \"leagueoflegends\",\n",
        "      \"lgbt\", \"Libertarian\", \"linguistics\", \"MensRights\", \"news\", \"offbeat\", \"PoliticalCompassMemes\", \"politics\",\n",
        "      \"teenagers\", \"TrueReddit\", \"TwoXChromosomes\", \"wallstreetbets\", \"worldnews\"]\n",
        "\n",
        "\n",
        "def create_trainings_data(subreddit_name, eval_split=0.8):\n",
        "    \"\"\"\n",
        "    Generates text files that are used for training the evaluation model. A training file with the naming\n",
        "    training_[subredditname].txt and an evaluation file with the naming eval_[subredditname].txt are created.\n",
        "    :param subreddit_name: Name of the subreddit for which an evaluation model is to be trained\n",
        "    :type subreddit_name: str\n",
        "    :param eval_split: Training split size. By default 0.8\n",
        "    :type eval_split float\n",
        "    :param data_dir: Directory in which the trainings data should be saved\n",
        "    :type data_dir: str\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "    comment_data = pandas.io.parsers.read_csv(csv_comments_path)\n",
        "    if use_submisson_data:\n",
        "        submission_data = pandas.io.parsers.read_csv(csv_comments_path)\n",
        "    query = '`subreddit` == ' + '\"' + subreddit_name + '\"'\n",
        "    comments_data_subreddit = comment_data.query(query)\n",
        "    if use_submisson_data:\n",
        "        submission_data_subreddit = submission_data.query(query)\n",
        "    number_of_comments = 1\n",
        "    for index, row in comments_data_subreddit.iterrows():\n",
        "        if row['body'] == \"[deleted]\":\n",
        "            continue\n",
        "        if row['perplexity'] < min_perplexity:\n",
        "            continue\n",
        "        number_of_comments += 1\n",
        "        texts.append(row['body'].strip() + ' <|endoftext|>')\n",
        "    if use_submisson_data:\n",
        "        for index, row in submission_data_subreddit.iterrows():\n",
        "            texts.append(row['title'].strip() + ' <|endoftext|>')\n",
        "    # split data in training and eval\n",
        "    split_index = int(len(texts) * eval_split)\n",
        "    trainings_data = texts[:split_index]\n",
        "    eval_data = texts[split_index:]\n",
        "    # write to file\n",
        "    training_text = \"\"\n",
        "    for sentence in trainings_data:\n",
        "        training_text += sentence + \"\\n\"\n",
        "    with open(f\"{data_dir}/training_{subreddit_name}.txt\", \"w\") as outfile:\n",
        "        outfile.write(training_text)\n",
        "    eval_text = \"\"\n",
        "    for sentence in eval_data:\n",
        "        eval_text += sentence + \"\\n\"\n",
        "    with open(f\"{data_dir}/eval_{subreddit_name}.txt\", \"w\") as outfile:\n",
        "        outfile.write(eval_text)\n",
        "    return number_of_comments\n",
        "\n",
        "\n",
        "def train_model(subreddit_name, model_dir=eval_models_dir):\n",
        "    \"\"\"\n",
        "    Train a model for the given subreddit\n",
        "    :param subreddit_name: Name of the subreddit for which an evaluation model is to be trained\n",
        "    :param data_dir: Folder in which the training data is located\n",
        "    :param model_dir: Folder in which the models should be saved\n",
        "    \"\"\"\n",
        "    train_path = f\"{data_dir}/training_{subreddit_name}.txt\"\n",
        "    eval_path = f\"{data_dir}/eval_{subreddit_name}.txt\"\n",
        "\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # currently only models that support the gpt2 tokenizer can be used\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "    output_dir = f\"./{model_dir}/{subreddit_name}\"\n",
        "    # create output folder if it not exists\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,  # The output directory\n",
        "        overwrite_output_dir=True,  # overwrite the content of the output directory\n",
        "        num_train_epochs=trainings_epochs,  # number of training epochs\n",
        "        per_device_train_batch_size=4,  # batch size for training\n",
        "        per_device_eval_batch_size=4,  # batch size for evaluation\n",
        "        eval_steps=400,  # Number of update steps between two evaluations.\n",
        "        save_steps=800,  # after # steps model is saved\n",
        "        warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
        "    )\n",
        "\n",
        "    dataset = datasets.load_dataset(\"text\", data_files={\"train\": train_path, \"test\": eval_path}, sample_by=\"line\")\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        # Remove empty lines\n",
        "        examples[\"text\"] = [line for line in examples[\"text\"] if len(line) > 0 and not line.isspace()]\n",
        "        return tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "        )\n",
        "\n",
        "    tokenized_dataset = dataset.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        num_proc=4,\n",
        "        remove_columns=[\"text\"],\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=tokenized_dataset[\"train\"],\n",
        "        eval_dataset=tokenized_dataset[\"test\"]\n",
        "    )\n",
        "    trainer.train()\n",
        "    trainer.save_model()\n",
        "\n",
        "\n",
        "# create folders that will be used later in the process\n",
        "if not os.path.exists(eval_models_dir):\n",
        "    os.makedirs(eval_models_dir)\n",
        "if not os.path.exists(trainings_data_dir):\n",
        "    os.makedirs(trainings_data_dir)\n",
        "\n",
        "# create the trainings and evaluation data for the evaluation models\n",
        "training_data_bar = tqdm(evaluation_models)\n",
        "data_stats = {}\n",
        "for subreddit in training_data_bar:\n",
        "    training_data_bar.set_description(f\"Processing {subreddit}\")\n",
        "    data_stats[subreddit] = create_trainings_data(subreddit_name=subreddit)\n",
        "print(f\"Data distribution {str(data_stats)}\")\n",
        "\n",
        "\n",
        "# train the evaluation models\n",
        "training_model_bar = tqdm(evaluation_models)\n",
        "for subreddit in tqdm(evaluation_models):\n",
        "    training_model_bar.set_description(f\"Training model for {subreddit}\")\n",
        "    train_model(subreddit_name=subreddit)\n"
      ],
      "metadata": {
        "id": "rWTd5nSCFm1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üë∑ Prepare the evaluation\n",
        "Optional. You only need it if you want to examine the style-specific perplexities of the data."
      ],
      "metadata": {
        "id": "YLHwKTPPDDwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "models = [\"antiwork\", \"atheism\", \"Conservative\", \"conspiracy\", \"dankmemes\",\n",
        "          \"gaybros\", \"leagueoflegends\", \"lgbt\", \"Libertarian\", \"linguistics\",\n",
        "          \"MensRights\", \"news\", \"offbeat\", \"PoliticalCompassMemes\",\n",
        "          \"politics\", \"teenagers\", \"TrueReddit\", \"TwoXChromosomes\",\n",
        "          \"wallstreetbets\", \"worldnews\"]\n",
        "\n",
        "def get_perplexity(model, encodings):\n",
        "    max_length = model.config.n_positions\n",
        "    stride = 512\n",
        "    seq_len = encodings.input_ids.size(1)\n",
        "\n",
        "    nlls = []\n",
        "    prev_end_loc = 0\n",
        "    for begin_loc in range(0, seq_len, stride):\n",
        "        end_loc = min(begin_loc + max_length, seq_len)\n",
        "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
        "        input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
        "        target_ids = input_ids.clone()\n",
        "        target_ids[:, :-trg_len] = -100\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, labels=target_ids)\n",
        "\n",
        "            # loss is calculated using CrossEntropyLoss which averages over input tokens.\n",
        "            # Multiply it with trg_len to get the summation instead of average.\n",
        "            # We will take average over all the tokens to get the true average\n",
        "            # in the last step of this example.\n",
        "            neg_log_likelihood = outputs.loss * trg_len\n",
        "\n",
        "        nlls.append(neg_log_likelihood)\n",
        "\n",
        "        prev_end_loc = end_loc\n",
        "        if end_loc == seq_len:\n",
        "            break\n",
        "\n",
        "    return torch.exp(torch.stack(nlls).sum() / end_loc).item()\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    # check if cuda is available and set device\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    return torch.device('cpu')\n",
        "\n",
        "\n",
        "class StyleSpecificPerplexity:\n",
        "\n",
        "    def __init__(self):\n",
        "        # load config\n",
        "        self.config = {\"evaluation_models\": models, \"eval_model_dir\": 'eval_models'}\n",
        "        self.models = {}\n",
        "        for model_name in self.config[\"evaluation_models\"]:\n",
        "            device = get_device()\n",
        "            # load tokenizer an model\n",
        "            model = AutoModelForCausalLM.from_pretrained(f\"{self.config['eval_model_dir']}/{model_name}\").to(device)\n",
        "            tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "            self.models[model_name] = {\"model\": model, \"tokenizer\": tokenizer}\n",
        "\n",
        "    def calculate_perplexity(self, model_name, input_texts=[]):\n",
        "        if model_name not in self.models:\n",
        "            raise Exception(f\"No model for style {model_name}\")\n",
        "\n",
        "        # load model and tokenizer\n",
        "        model = self.models[model_name][\"model\"]\n",
        "        tokenizer = self.models[model_name][\"tokenizer\"]\n",
        "\n",
        "        perplexities = []\n",
        "        for input_text in input_texts:\n",
        "            # encode text\n",
        "            device = get_device()\n",
        "            encodings = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "            # calculate perplexity\n",
        "            perplexity = get_perplexity(model, encodings)\n",
        "\n",
        "            # If no perplexity could be calculated (for example because the input contains only one word),\n",
        "            # do not save. Result would be NaN. This means that no more total perplexity can be calculated.\n",
        "            if math.isnan(perplexity):\n",
        "                continue\n",
        "\n",
        "            perplexities.append(perplexity)\n",
        "        # return perplexity values\n",
        "        return perplexities\n"
      ],
      "metadata": {
        "id": "cDW2voWNDENw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ Evaluate the output of the models"
      ],
      "metadata": {
        "id": "uBzJk_RpCeUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set this variable to True if you want to calculate the style specific perplexities.\n",
        "# Note that you have to execute the upper two cells for this.\n",
        "calculate_style_specific_perplexities = False\n",
        "\n",
        "# This variable is used to create a suitable name for the output file\n",
        "result_data_model_name = 't5-base'\n",
        "\n",
        "# Change this variable to evaluate other results\n",
        "result_data_filename = 't5-base_results.json'\n",
        "\n",
        "result_data_file = open(result_data_filename)\n",
        "result_data_json = json.load(result_data_file)\n",
        "\n",
        "references = []\n",
        "predictions = []\n",
        "subreddits = []\n",
        "for res_object in result_data_json[\"data\"]:\n",
        "    if len(res_object['input']) == 0:\n",
        "        continue\n",
        "    references.append([res_object['input']])\n",
        "    predictions.append(res_object['output'])\n",
        "\n",
        "# create a flat list of references, needed for the calculation of BERTScore\n",
        "flat_references = [item for sublist in references for item in sublist]\n",
        "\n",
        "print(\"About to calculate dataset label scores\")\n",
        "\n",
        "# load models\n",
        "bert_score_model = evaluate.load(\"bertscore\")\n",
        "perplexity_model = evaluate.load(\"perplexity\", module_type=\"measurement\")\n",
        "bleu_model = evaluate.load(\"chrf\")\n",
        "\n",
        "res_per_subreddit = {}\n",
        "for res_object in result_data_json['data']:\n",
        "    subreddit = res_object['subreddit']\n",
        "    if subreddit not in res_per_subreddit:\n",
        "        res_per_subreddit[subreddit] = []\n",
        "    res_per_subreddit[subreddit].append(res_object['output'])\n",
        "\n",
        "tmp_scores = {}\n",
        "# create a perplexity ranking of all subreddits (to check if the target subreddit has the lowest perplexity)\n",
        "tmp_scores['overall_perplexity'] = []\n",
        "if calculate_style_specific_perplexities:\n",
        "  style_specific_perplexity = StyleSpecificPerplexity()\n",
        "  print('Create all subreddit perplexities:')\n",
        "  for subreddit, subreddit_data in tqdm(res_per_subreddit.items()):\n",
        "      for subreddit_model in config['evaluation_models']:\n",
        "          perplexities = style_specific_perplexity.calculate_perplexity(subreddit_model, subreddit_data)\n",
        "          tmp_scores['overall_perplexity'].append({'target_subreddit': subreddit,\n",
        "                                                 'subreddit_model': subreddit_model,\n",
        "                                                 'perplexity': np.mean(perplexities)})\n",
        "\n",
        "  # rate style specific perplexity\n",
        "  tmp_scores['style_specific_perplexity'] = {}\n",
        "  print('Create target subreddit perplexities:')\n",
        "  for subreddit, subreddit_data in tqdm(res_per_subreddit.items()):\n",
        "      perplexities = style_specific_perplexity.calculate_perplexity(subreddit, subreddit_data)\n",
        "      tmp_scores['style_specific_perplexity'][subreddit] = np.mean(perplexities)\n",
        "\n",
        "\n",
        "\n",
        "# calculate scores\n",
        "print('Create bert_score')\n",
        "tmp_scores['bert_score'] = bert_score_model.compute(predictions=predictions, references=flat_references,\n",
        "                                                    lang=\"en\", model_type=\"microsoft/deberta-xlarge-mnli\",\n",
        "                                                    device='cuda')\n",
        "\n",
        "print('Create chrF++')\n",
        "tmp_scores['bleu_score'] = bleu_model.compute(predictions=predictions, references=references, word_order=2, )\n",
        "\n",
        "print('Create GPT-2 perplexities')\n",
        "tmp_scores['perplexity_score'] = perplexity_model.compute(data=predictions, model_id='gpt2', device='cpu')\n",
        "\n",
        "print(\"Dataset label scores calculated\")\n",
        "\n",
        "# BERT\n",
        "bert_scores = tmp_scores['bert_score']\n",
        "bert_f1_mean_score = sum(bert_scores['f1']) / len(bert_scores['f1'])\n",
        "bert_precision_mean_score = sum(bert_scores['precision']) / len(bert_scores['precision'])\n",
        "bert_recall_mean_score = sum(bert_scores['recall']) / len(bert_scores['recall'])\n",
        "\n",
        "bert_score = {'mean_f1': bert_f1_mean_score, 'mean_precision': bert_precision_mean_score,\n",
        "              'mean_recall': bert_recall_mean_score}\n",
        "# Perplexity\n",
        "perplexity_score = tmp_scores['perplexity_score']\n",
        "median_perplexity = np.median(perplexity_score['perplexities'])\n",
        "variance_perplexity = np.var(perplexity_score['perplexities'])\n",
        "perplexity = {'perplexity_median': median_perplexity, 'perplexity_variance': variance_perplexity,\n",
        "              'perplexity_mean': perplexity_score['mean_perplexity']}\n",
        "# save results in object\n",
        "result_obj = {'BLEU': tmp_scores['bleu_score'], 'Perplexity': perplexity,\n",
        "              'BERTScore': bert_score,\n",
        "              'overall_perplexity': tmp_scores['overall_perplexity']}\n",
        "\n",
        "if calculate_style_specific_perplexities:\n",
        "  result_obj['Style_Specific_Perplexity'] = tmp_scores['style_specific_perplexity']\n",
        "\n",
        "# save result object\n",
        "with open(f\"{result_data_model_name}_eval_results.json\", \"w\") as r:\n",
        "    json.dump(result_obj, r)\n"
      ],
      "metadata": {
        "id": "clHSmDDfCpwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Visualize data"
      ],
      "metadata": {
        "id": "xRYGSgWF6kPK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the accuracy of style transfers. For this, the perplexity is evaluated by subreddit trained on the styles. If the perplexity of the target subreddit is low, this should be a good sign.\n",
        "\n",
        "You can change the filename of the files to be visualized in the code"
      ],
      "metadata": {
        "id": "hPykqz2W69nW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# File to be visualized\n",
        "file_name = 't5-flan_epoch4_results.json'\n",
        "\n",
        "file = open(file_name)\n",
        "data = json.load(file)['overall_perplexity']\n",
        "\n",
        "# Get unique target subreddits\n",
        "target_subreddits = set(entry[\"target_subreddit\"] for entry in data)\n",
        "\n",
        "for subreddit in target_subreddits:\n",
        "    # Filter data for the current target subreddit\n",
        "    filtered_data = [entry for entry in data if entry[\"target_subreddit\"] == subreddit]\n",
        "\n",
        "    # Sort the filtered data based on perplexity in ascending order\n",
        "    sorted_data = sorted(filtered_data, key=lambda x: x[\"perplexity\"])[:5]  # Get the five lowest perplexities\n",
        "\n",
        "    # Extract the subreddit models and perplexities\n",
        "    subreddit_models = [entry[\"subreddit_model\"] for entry in sorted_data]\n",
        "    perplexities = [round(entry[\"perplexity\"], 2) for entry in sorted_data]  # Round perplexities to two decimal places\n",
        "\n",
        "    # Increase the figure size\n",
        "    plt.figure(figsize=(9, 12))  # Adjust the values as needed\n",
        "\n",
        "    # Plotting the bar graph\n",
        "    plt.bar(subreddit_models, perplexities)\n",
        "\n",
        "    # Adding labels and title\n",
        "    plt.xlabel(\"Subreddit Model\")\n",
        "    plt.ylabel(\"Perplexity\")\n",
        "    plt.title(f\"Five Lowest Perplexities for {subreddit}\")\n",
        "\n",
        "    # Adding perplexity values on top of each bar\n",
        "    for i, v in enumerate(perplexities):\n",
        "        plt.text(i, v, str(v), ha='center', va='bottom')\n",
        "\n",
        "    # Rotating the x-axis labels for better visibility\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Displaying the bar graph\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "7Lb4Peit6r4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Present the BERTScores and/or chrF++ and/or the perplexity of the generated texts. What you want to display, you can set in the code"
      ],
      "metadata": {
        "id": "qfgL_OZ77TXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# which metrics are to be displayed\n",
        "show_bert_score = True # BERTScore\n",
        "show_chrf = True # chrF++\n",
        "show_ppl = True # Perplexity\n",
        "\n",
        "# files\n",
        "files1 = ['bart_epoch0_results.json', 'bart_epoch1_results.json', 'bart_epoch2_results.json',\n",
        "          'bart_epoch3_results.json', 'bart_epoch4_results.json']\n",
        "\n",
        "files2 = ['t5_epoch0_results.json', 't5_epoch1_results.json', 't5_epoch2_results.json',\n",
        "          'results/t5_epoch3_results.json', 'results/t5_epoch4_results.json']\n",
        "\n",
        "files3 = ['gpt-neo_epoch0_results.json', 'gpt-neo_epoch1_results.json',\n",
        "          'gpt-neo_epoch2_results.json', 'gpt-neo_epoch3_results.json',\n",
        "          'gpt-neo_epoch4_results.json']\n",
        "\n",
        "files4 = ['t5-flan_epoch0_results.json', 't5-flan_epoch1_results.json',\n",
        "          't5-flan_epoch2_results.json',\n",
        "          't5-flan_epoch3_results.json', 't5-flan_epoch4_results.json']\n",
        "\n",
        "files_data1 = []\n",
        "files_data2 = []\n",
        "files_data3 = []\n",
        "files_data4 = []\n",
        "\n",
        "for file_path in files1:\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "        files_data1.append(data)\n",
        "\n",
        "for file_path in files2:\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "        files_data2.append(data)\n",
        "\n",
        "for file_path in files3:\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "        files_data3.append(data)\n",
        "\n",
        "for file_path in files4:\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "        files_data4.append(data)\n",
        "\n",
        "# Extract data for plotting\n",
        "bleu_scores1 = [data[\"BLEU\"][\"score\"] for data in files_data1]\n",
        "bleu_scores2 = [data[\"BLEU\"][\"score\"] for data in files_data2]\n",
        "bleu_scores3 = [data[\"BLEU\"][\"score\"] for data in files_data3]\n",
        "bleu_scores4 = [data[\"BLEU\"][\"score\"] for data in files_data4]\n",
        "\n",
        "bert_scores1 = [data[\"BERTScore\"][\"mean_f1\"] for data in files_data1]\n",
        "bert_scores2 = [data[\"BERTScore\"][\"mean_f1\"] for data in files_data2]\n",
        "bert_scores3 = [data[\"BERTScore\"][\"mean_f1\"] for data in files_data3]\n",
        "bert_scores4 = [data[\"BERTScore\"][\"mean_f1\"] for data in files_data4]\n",
        "\n",
        "perplexity_median1 = [data[\"Perplexity\"][\"perplexity_mean\"] for data in files_data1]\n",
        "perplexity_median2 = [data[\"Perplexity\"][\"perplexity_mean\"] for data in files_data2]\n",
        "perplexity_median3 = [data[\"Perplexity\"][\"perplexity_mean\"] for data in files_data3]\n",
        "perplexity_median4 = [data[\"Perplexity\"][\"perplexity_mean\"] for data in files_data4]\n",
        "\n",
        "# Generate x-axis values (file numbers)\n",
        "file_numbers1 = range(1, len(files_data1) + 1)\n",
        "file_numbers2 = range(1, len(files_data2) + 1)\n",
        "file_numbers3 = range(1, len(files_data3) + 1)\n",
        "file_numbers4 = range(1, len(files_data4) + 1)\n",
        "\n",
        "# Plotting the data\n",
        "\n",
        "if show_chrf:\n",
        "  plt.plot(file_numbers1, [0.83, 0.83, 0.83, 0.83, 0.83], label='davinci-003', color='red')\n",
        "\n",
        "if show_bert_score:\n",
        "  plt.plot(file_numbers1, [0.36, 0.36, 0.36, 0.36, 0.36], label='davinci-003', color='red')\n",
        "\n",
        "if show_ppl:\n",
        "  plt.plot(file_numbers1, perplexity_median1, label='bart-base')\n",
        "  plt.plot(file_numbers2, perplexity_median2, label='t5-base')\n",
        "  plt.plot(file_numbers4, perplexity_median4, label='flan-t5-base')\n",
        "  plt.plot(file_numbers3, perplexity_median3, label='gpt-neo-2.7B')\n",
        "\n",
        "\n",
        "if show_bert_score:\n",
        "  plt.plot(file_numbers1, bleu_scores1, label='bart-base')\n",
        "  plt.plot(file_numbers2, bleu_scores2, label='t5-base')\n",
        "  plt.plot(file_numbers4, bleu_scores4, label='flan-t5-base')\n",
        "  plt.plot(file_numbers3, bleu_scores3, label='gpt-neo-2.7B')\n",
        "\n",
        "if show_chrf:\n",
        "  plt.plot(file_numbers3, bert_scores3, label='gpt-neo-2.7B')\n",
        "  plt.plot(file_numbers2, bert_scores2, label='t5-base')\n",
        "  plt.plot(file_numbers4, bert_scores4, label='flan-t5-base')\n",
        "  plt.plot(file_numbers1, bert_scores1, label='bart-base')\n",
        "\n",
        "\n",
        "# Set the plot title and labels\n",
        "plt.title('Development of BERTScore over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Value')\n",
        "\n",
        "# Show a legend\n",
        "plt.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "29KKz_8A7XJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The perplexities of the generated texts of the models in the course of the training."
      ],
      "metadata": {
        "id": "CAGBpxUN_eLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the number of epochs\n",
        "num_epochs = 5\n",
        "\n",
        "# Define the models and their corresponding file patterns\n",
        "models = {\n",
        "    't5': 'results/t5_epoch{}_results.json',\n",
        "    'flan-t5': 'results/t5-flan_epoch{}_results.json',\n",
        "     # 'gpt-neo-2.7B': 'results/gpt-neo_epoch{}_results.json',\n",
        "    'bart-base': 'results/bart_epoch{}_results.json'\n",
        "}\n",
        "\n",
        "# Initialize a dictionary to store perplexity values for each model\n",
        "model_perplexities = {}\n",
        "\n",
        "# Iterate over each model\n",
        "for model, file_pattern in models.items():\n",
        "    # Initialize a list to store perplexity values for the current model\n",
        "    perplexities = []\n",
        "\n",
        "    # Iterate over each epoch\n",
        "    for epoch in range(num_epochs):\n",
        "        # Read the file for the current epoch and model\n",
        "        file_path = file_pattern.format(epoch)\n",
        "        with open(file_path, 'r') as file:\n",
        "            data = json.load(file)\n",
        "            perplexity = data['Perplexity']['perplexity_mean']\n",
        "            # perplexity = data['Perplexity']['perplexity_median']\n",
        "            perplexities.append(perplexity)\n",
        "\n",
        "    # Store the perplexities for the current model in the dictionary\n",
        "    model_perplexities[model] = perplexities\n",
        "\n",
        "# Generate x-axis values (epochs)\n",
        "epochs = range(1, num_epochs + 1)\n",
        "\n",
        "# Plotting the data for each model\n",
        "for model, perplexities in model_perplexities.items():\n",
        "    plt.plot(epochs, perplexities, label=model)\n",
        "\n",
        "# mean values of text-davinci-003 (Baseline-Model)\n",
        "plt.plot([1, 2, 3, 4, 5], [151.7, 151.7, 151.7, 151.7, 151.7], label='davinci-003', color='red')\n",
        "\n",
        "# median values of text-davinci-003 (Baseline-Model)\n",
        "# plt.plot([1, 2, 3, 4, 5], [150.32, 150.32, 150.32, 150.32, 150.32], label='davinci-003', color='red')\n",
        "\n",
        "\n",
        "# Set the plot title and labels\n",
        "plt.title('Development of mean perplexity over epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Mean Perplexity')\n",
        "\n",
        "# Add a legend to distinguish the models\n",
        "plt.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q8mKs4t9_lOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The style specific perplexities of the generated texts of the models in the course of the training."
      ],
      "metadata": {
        "id": "dkDCTICKAQ6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the number of epochs\n",
        "num_epochs = 5\n",
        "\n",
        "# Define the models and their corresponding file patterns\n",
        "models = {\n",
        "    't5-flan': 'results/t5-flan_epoch{}_results.json',\n",
        "    #'gpt-neo': 'results/gpt-neo_epoch{}_results.json',\n",
        "    #'t5': 'results/t5_epoch{}_results.json',\n",
        "    #'bart': 'results/bart_epoch{}_results.json'\n",
        "}\n",
        "\n",
        "# Define the target subreddits and their corresponding colors\n",
        "subreddits = {\n",
        "    'TrueReddit': 'blue',\n",
        "    'TwoXChromosomes': 'green',\n",
        "    'wallstreetbets': 'red',\n",
        "    'worldnews': 'purple'\n",
        "}\n",
        "\n",
        "# Define the file path for the baseline data\n",
        "baseline_file = 'results/davinci-003_results.json'\n",
        "\n",
        "# Initialize a dictionary to store perplexity values for each model-subreddit combination\n",
        "model_perplexities = {}\n",
        "\n",
        "# Iterate over each model\n",
        "for model, file_pattern in models.items():\n",
        "    # Initialize a dictionary to store perplexity values for the current model\n",
        "    model_perplexities[model] = {}\n",
        "\n",
        "    # Iterate over each subreddit\n",
        "    for subreddit, color in subreddits.items():\n",
        "        # Initialize a list to store perplexity values for the current model-subreddit combination\n",
        "        perplexities = []\n",
        "\n",
        "        # Iterate over each epoch\n",
        "        for epoch in range(num_epochs):\n",
        "            # Read the file for the current epoch, model, and subreddit\n",
        "            file_path = file_pattern.format(epoch)\n",
        "            with open(file_path, 'r') as file:\n",
        "                data = json.load(file)\n",
        "                perplexity = data['Style_Specific_Perplexity'][subreddit]\n",
        "                perplexities.append(perplexity)\n",
        "\n",
        "        # Store the perplexities for the current model-subreddit combination in the dictionary\n",
        "        model_perplexities[model][subreddit] = perplexities\n",
        "\n",
        "# Read the baseline data from the file\n",
        "with open(baseline_file, 'r') as file:\n",
        "    baseline_data = json.load(file)\n",
        "\n",
        "# Extract the necessary information from the baseline data\n",
        "baseline_perplexities = baseline_data['Style_Specific_Perplexity']\n",
        "\n",
        "# Generate x-axis values (epochs)\n",
        "epochs = range(1, num_epochs + 1)\n",
        "\n",
        "# Plotting the data for each model-subreddit combination\n",
        "for model, perplexities in model_perplexities.items():\n",
        "    for subreddit, values in perplexities.items():\n",
        "        color = subreddits[subreddit]\n",
        "        plt.plot(epochs, values, marker='o', label=f'{model}-{subreddit}', color=color)\n",
        "\n",
        "\n",
        "# Set the plot title and labels\n",
        "plt.title('flan-t5-base: Development of Style-Specific Perplexity over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Style-Specific Perplexity')\n",
        "\n",
        "# Add a legend to distinguish the model-subreddit and baseline combinations\n",
        "plt.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FHVSzuKkAJBR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
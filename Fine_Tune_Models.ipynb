{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ‚öôÔ∏è Install Libraries and Download Dataset\n",
        "\n",
        "After this step, the runtime must be restarted."
      ],
      "metadata": {
        "id": "Mz3OFn9dQd1v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDNDG_TXPlX-"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "!pip install simplet5 evaluate sacrebleu tqdm spacy matplotlib openai zenodo-get bert_score tensorflow\n",
        "\n",
        "!zenodo_get 10.5281/zenodo.8023142"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìù Initialize config"
      ],
      "metadata": {
        "id": "Q6K9LpV4Sp3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load config\n",
        "config = {\n",
        "    \"t5_models\": [\"t5-small\", \"t5-base\", \"google/flan-t5-base\", \"google/t5-efficient-small-el8-dl2\",\n",
        "      \"google/t5-efficient-small-el16-dl2\", \"google/t5-efficient-small-dm2000\", \"google/t5-v1_1-base\",\n",
        "      \"google/t5-small-lm-adapt\"],\n",
        "    \"bart_models\": [\"facebook/bart-base\", \"facebook/mbart-large-50\"],\n",
        "    \"trainings_data_path\": \"training_labeled_with_style_samples.json\",\n",
        "    \"eval_data_path\": \"eval_labeled_with_style_samples.json\",\n",
        "    \"trainings_params\": {\n",
        "      \"max_source_length\": 2048,\n",
        "      \"max_target_length\": 512,\n",
        "      \"use_gpu\": True\n",
        "    },\n",
        "    \"prompts\": {\n",
        "      \"intro_samples\": \"Here a example sentences: \",\n",
        "      \"intro_input_sentence\": \"Here is a sentence: \",\n",
        "      \"intro_output_sentence\": \"Here is a rewrite of this sentence according to the example sentences: \"\n",
        "    }\n",
        "  }\n",
        "\n",
        "print('Done!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDpS9v93SyGV",
        "outputId": "faf12950-71ce-428c-900a-79d0d450162a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üèãÔ∏è Train T5"
      ],
      "metadata": {
        "id": "gfCPXLvuQ4-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from simplet5 import SimpleT5\n",
        "\n",
        "TRAIN_REVERSE_PROMPT = False\n",
        "\n",
        "\n",
        "def load_training_and_eval_data(eval_split=0.8):\n",
        "    # load trainings data\n",
        "    f = open(config[\"trainings_data_path\"])\n",
        "    data = json.load(f)\n",
        "    # split data in training and eval data\n",
        "    split_index = int(len(data['data']) * eval_split)\n",
        "    training_data = data['data'][:split_index]\n",
        "    eval_data = data['data'][split_index:]\n",
        "    return training_data, eval_data\n",
        "\n",
        "\n",
        "def create_trainings_prompts(data):\n",
        "    train_data = {'source_text': [], 'target_text': []}\n",
        "    for trainings_object in data:\n",
        "        trainings_prompt = config[\"prompts\"][\"intro_samples\"] + \" \"\n",
        "        for sample in trainings_object[\"style_samples\"]:\n",
        "            trainings_prompt += \"{\" + sample + \"} \"\n",
        "        trainings_prompt += config[\"prompts\"][\"intro_input_sentence\"] + \"{\" + trainings_object['input_sentence'] + \"} \"\n",
        "        trainings_prompt += config[\"prompts\"][\"intro_output_sentence\"] + \"{\"\n",
        "        train_data['source_text'].append(trainings_prompt)\n",
        "        train_data['target_text'].append(trainings_object['result_sentence'] + '}')\n",
        "    return train_data\n",
        "\n",
        "\n",
        "def create_trainings_prompts_reverse(data):\n",
        "    train_data = {'source_text': [], 'target_text': []}\n",
        "    for trainings_object in data:\n",
        "        trainings_prompt = config[\"prompts\"][\"intro_input_sentence\"] + \" \" + trainings_object['input_sentence'] + \" \"\n",
        "        trainings_prompt += config[\"prompts\"][\"intro_samples\"] + \" \"\n",
        "        for sample in trainings_object[\"style_samples\"]:\n",
        "            trainings_prompt += \"{\" + sample + \"} \"\n",
        "        trainings_prompt += config[\"prompts\"][\"intro_output_sentence\"] + \" \"\n",
        "        train_data['source_text'].append(trainings_prompt)\n",
        "        train_data['target_text'].append(trainings_object['result_sentence'] + ' ')\n",
        "    return train_data\n",
        "\n",
        "\n",
        "def create_trainings_data():\n",
        "    training_data, eval_data = load_training_and_eval_data()\n",
        "    training_prompts = create_trainings_prompts(training_data)\n",
        "    eval_prompts = create_trainings_prompts(eval_data)\n",
        "    training_df = pd.DataFrame(data=training_prompts)\n",
        "    eval_df = pd.DataFrame(data=eval_prompts)\n",
        "    return training_df, eval_df\n",
        "\n",
        "\n",
        "def create_trainings_data_reverse():\n",
        "    training_data, eval_data = load_training_and_eval_data()\n",
        "    training_prompts = create_trainings_prompts_reverse(training_data)\n",
        "    eval_prompts = create_trainings_prompts(eval_data)\n",
        "    training_df = pd.DataFrame(data=training_prompts)\n",
        "    eval_df = pd.DataFrame(data=eval_prompts)\n",
        "    return training_df, eval_df\n",
        "\n",
        "\n",
        "def train_model(training_df, eval_df, model_name_param, save_name, trainings_params):\n",
        "    output_dir = f\"prod_{model_name_param}_{save_name}\"\n",
        "    print(f\"Train {model_name_param} {save_name}\")\n",
        "    model = SimpleT5()\n",
        "    model.from_pretrained(\"t5\", model_name_param)\n",
        "    model.train(train_df=training_df,  # pandas dataframe with 2 columns: source_text & target_text\n",
        "                eval_df=eval_df,  # pandas dataframe with 2 columns: source_text & target_text\n",
        "                source_max_token_len=trainings_params['max_source_length'],\n",
        "                target_max_token_len=trainings_params['max_target_length'],\n",
        "                batch_size=4,\n",
        "                max_epochs=5,\n",
        "                use_gpu=trainings_params['use_gpu'],\n",
        "                outputdir=output_dir,\n",
        "                early_stopping_patience_epochs=0,\n",
        "                precision=32\n",
        "                )\n",
        "\n",
        "\n",
        "training_dataframe, eval_dataframe = create_trainings_data()\n",
        "training_dataframe_reverse, eval_dataframe_reverse = create_trainings_data_reverse()\n",
        "for model_name in config['t5_models']:\n",
        "    train_model(training_dataframe, eval_dataframe, model_name, 'default', config['trainings_params'])\n",
        "    if TRAIN_REVERSE_PROMPT:\n",
        "      train_model(training_dataframe_reverse, eval_dataframe_reverse, model_name, 'reverse', config['trainings_params'])\n"
      ],
      "metadata": {
        "id": "4p7lMUOwRGbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üèãÔ∏è Train BART"
      ],
      "metadata": {
        "id": "x-PXB68ISUhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, AutoTokenizer\n",
        "\n",
        "trainings_data_file = open(config['trainings_data_path'])\n",
        "training_data = json.load(trainings_data_file)['data']\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "sep_token = tokenizer.sep_token\n",
        "bos_token = tokenizer.bos_token\n",
        "eos_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "def create_trainings_for_bert(array):\n",
        "    result_text = []\n",
        "    for trainings_object in array:\n",
        "        trainings_prompt = \"\"\n",
        "        for sample in trainings_object['style_samples']:\n",
        "            trainings_prompt += bos_token + sample + sep_token\n",
        "        trainings_prompt += bos_token + trainings_object['input_sentence'] + eos_token\n",
        "        result_sentence = bos_token + trainings_object['result_sentence'] + eos_token\n",
        "        result_text.append((trainings_prompt, result_sentence))\n",
        "    return result_text\n",
        "\n",
        "\n",
        "def train_model(model_name, training_dataset):\n",
        "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "\n",
        "    training_data = [[tokenizer.encode(input_text, return_tensors='pt').to(device),\n",
        "                      tokenizer.encode(output_text, return_tensors='pt').to(device)]\n",
        "                     for input_text, output_text in training_dataset]\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "    for epoch in range(5):\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        with tqdm(total=len(training_data), desc=f\"Epoch {epoch + 1}\") as pbar:\n",
        "            for input_ids, output_ids in training_data:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Generate the output using the model\n",
        "                generated_ids = model(input_ids=input_ids, decoder_input_ids=output_ids[:, :-1]).logits\n",
        "\n",
        "                # Calculate the loss\n",
        "                loss = torch.nn.functional.cross_entropy(generated_ids.view(-1, generated_ids.size(-1)),\n",
        "                                                         output_ids[:, 1:].reshape(-1))\n",
        "\n",
        "                # Backpropagate and update the model\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                pbar.set_postfix({'loss': epoch_loss / len(training_data)})\n",
        "                pbar.update()\n",
        "\n",
        "        model.save_pretrained(f\"prod_{model_name}_epoch{epoch}\")\n",
        "\n",
        "\n",
        "training_dataset = create_trainings_for_bert(training_data)\n",
        "# train the models\n",
        "for model_name in config[\"bart_models\"]:\n",
        "    train_model(model_name=model_name, training_dataset=training_dataset)\n"
      ],
      "metadata": {
        "id": "VACrNhGQSdp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üèãÔ∏è Train GPT"
      ],
      "metadata": {
        "id": "arKOlnMJSTk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, GPTNeoForCausalLM\n",
        "\n",
        "trainings_data_file = open(config['trainings_data_path'])\n",
        "training_data = json.load(trainings_data_file)['data']\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "eos_token = tokenizer.eos_token\n",
        "\n",
        "def create_trainings_for_gpt_neo(array):\n",
        "    result_text = []\n",
        "    for trainings_object in array:\n",
        "        trainings_prompt = \"\"\n",
        "        for sample in trainings_object['style_samples']:\n",
        "            trainings_prompt += sample + '\\n'\n",
        "        trainings_prompt += trainings_object['input_sentence'] + eos_token\n",
        "        result_sentence = trainings_object['result_sentence'] + eos_token\n",
        "        result_text.append((trainings_prompt, result_sentence))\n",
        "    return result_text\n",
        "\n",
        "def train_model(training_dataset):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "    model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\").to(device)\n",
        "\n",
        "    training_data = [[tokenizer.encode(input_text, return_tensors='pt').to(device),\n",
        "                      tokenizer.encode(output_text, return_tensors='pt').to(device)]\n",
        "                     for input_text, output_text in training_dataset]\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "    for epoch in range(5):\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        with tqdm(total=len(training_data), desc=f\"Epoch {epoch + 1}\") as pbar:\n",
        "            for input_ids, output_ids in training_data:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Generate the output using the model\n",
        "                max_length = max(input_ids.size(1), output_ids.size(1))\n",
        "                input_ids = torch.nn.functional.pad(input_ids, (0, max_length - input_ids.size(1)))\n",
        "                labels = torch.nn.functional.pad(output_ids, (0, max_length - output_ids.size(1)))\n",
        "                generated_ids = model(input_ids=input_ids, labels=labels).logits\n",
        "\n",
        "                # Pad generated_ids and labels to the same length\n",
        "                max_length = max(generated_ids.size(1), labels.size(1))\n",
        "                generated_ids = torch.nn.functional.pad(generated_ids, (0, max_length - generated_ids.size(1)))\n",
        "                labels = torch.nn.functional.pad(labels, (0, max_length - labels.size(1)))\n",
        "\n",
        "                # Calculate the loss\n",
        "                loss = torch.nn.functional.cross_entropy(generated_ids.view(-1, generated_ids.size(-1)),\n",
        "                                                         labels.view(-1))\n",
        "\n",
        "                # Backpropagate and update the model\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                pbar.set_postfix({'loss': epoch_loss / len(training_data)})\n",
        "                pbar.update()\n",
        "\n",
        "        model.save_pretrained(f\"prod_gpt-neo_epoch{epoch}\")\n",
        "\n",
        "training_dataset = create_trainings_for_gpt_neo(training_data)\n",
        "# train the models\n",
        "train_model(training_dataset=training_dataset)\n"
      ],
      "metadata": {
        "id": "WiCmE1Q1SeJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üèãÔ∏è Train BERT"
      ],
      "metadata": {
        "id": "JEm_w8TbTEKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertLMHeadModel, AdamW\n",
        "\n",
        "trainings_data_file = open(config['trainings_data_path'])\n",
        "training_data = json.load(trainings_data_file)['data']\n",
        "\n",
        "# Initialize BERT model and tokenizer\n",
        "model = BertLMHeadModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "sep_token = tokenizer.sep_token\n",
        "\n",
        "# Set up optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Set up training data (input sequences and target output sequences)\n",
        "def create_trainings_for_bert(array):\n",
        "    result_text = []\n",
        "    for trainings_object in array:\n",
        "        trainings_prompt = \"\"\n",
        "        for sample in trainings_object['style_samples']:\n",
        "            trainings_prompt += sep_token + sample + sep_token\n",
        "        trainings_prompt += sep_token + trainings_object['input_sentence'] + sep_token\n",
        "        result_sentence = sep_token + trainings_object['result_sentence'] + sep_token\n",
        "        result_text.append((trainings_prompt, result_sentence))\n",
        "    return result_text\n",
        "\n",
        "\n",
        "trainings_data = create_trainings_for_bert(training_data)\n",
        "\n",
        "# Set up device (GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 4\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for input_seq, target_seq in trainings_data:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Tokenize input sequence and target sequence\n",
        "        input_tokens = tokenizer.encode(input_seq, add_special_tokens=True, truncation=True, padding='max_length')\n",
        "        target_tokens = tokenizer.encode(target_seq, add_special_tokens=True, truncation=True, padding='max_length')\n",
        "\n",
        "        # Convert tokenized sequences to tensors\n",
        "        input_tensors = torch.tensor([input_tokens]).to(device)\n",
        "        target_tensors = torch.tensor([target_tokens]).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_tensors, labels=target_tensors)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(training_data)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {average_loss:.4f}\")\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    model.save_pretrained(f'bert_epoch{epoch}')\n"
      ],
      "metadata": {
        "id": "M4KOsjAwUEqd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}